---
title: "Multinomial Logit Model"
author: "Ting Yu(Sophia) Wang"
date: today
---


This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm. 


## 1. Likelihood for the Multi-nomial Logit (MNL) Model

Suppose we have $i=1,\ldots,n$ consumers who each select exactly one product $j$ from a set of $J$ products. The outcome variable is the identity of the product chosen $y_i \in \{1, \ldots, J\}$ or equivalently a vector of $J-1$ zeros and $1$ one, where the $1$ indicates the selected product. For example, if the third product was chosen out of 3 products, then either $y=3$ or $y=(0,0,1)$ depending on how we want to represent it. Suppose also that we have a vector of data on each product $x_j$ (eg, brand, price, etc.). 

We model the consumer's decision as the selection of the product that provides the most utility, and we'll specify the utility function as a linear function of the product characteristics:

$$ U_{ij} = x_j'\beta + \epsilon_{ij} $$

where $\epsilon_{ij}$ is an i.i.d. extreme value error term. 

The choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer $i$ chooses product $j$:

$$ \mathbb{P}_i(j) = \frac{e^{x_j'\beta}}{\sum_{k=1}^Je^{x_k'\beta}} $$

For example, if there are 3 products, the probability that consumer $i$ chooses product 3 is:

$$ \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{e^{x_1'\beta} + e^{x_2'\beta} + e^{x_3'\beta}} $$

A clever way to write the individual likelihood function for consumer $i$ is the product of the $J$ probabilities, each raised to the power of an indicator variable ($\delta_{ij}$) that indicates the chosen product:

$$ L_i(\beta) = \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} = \mathbb{P}_i(1)^{\delta_{i1}} \times \ldots \times \mathbb{P}_i(J)^{\delta_{iJ}}$$

Notice that if the consumer selected product $j=3$, then $\delta_{i3}=1$ while $\delta_{i1}=\delta_{i2}=0$ and the likelihood is:

$$ L_i(\beta) = \mathbb{P}_i(1)^0 \times \mathbb{P}_i(2)^0 \times \mathbb{P}_i(3)^1 = \mathbb{P}_i(3) = \frac{e^{x_3'\beta}}{\sum_{k=1}^3e^{x_k'\beta}} $$

The joint likelihood (across all consumers) is the product of the $n$ individual likelihoods:

$$ L_n(\beta) = \prod_{i=1}^n L_i(\beta) = \prod_{i=1}^n \prod_{j=1}^J \mathbb{P}_i(j)^{\delta_{ij}} $$

And the joint log-likelihood function is:

$$ \ell_n(\beta) = \sum_{i=1}^n \sum_{j=1}^J \delta_{ij} \log(\mathbb{P}_i(j)) $$



## 2. Simulate Conjoint Data

We will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a "no choice" option; each simulated respondent must select one of the 3 alternatives. 

Each alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from \$4 to \$32 in increments of \$4.

The part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer $i$ for hypothethical streaming service $j$ is 

$$
u_{ij} = (1 \times Netflix_j) + (0.5 \times Prime_j) + (-0.8*Ads_j) - 0.1\times Price_j + \varepsilon_{ij}
$$

where the variables are binary indicators and $\varepsilon$ is Type 1 Extreme Value (ie, Gumble) distributed.

The following code provides the simulation of the conjoint data.

:::: {.callout-note collapse="true"}
```{python}
import numpy as np
import pandas as pd
import itertools

np.random.seed(123)
brand = ["N", "P", "H"]  
ad = ["Yes", "No"]
price = np.arange(8, 33, 4)

profiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=["brand", "ad", "price"])
m = len(profiles)

b_util = {"N": 1.0, "P": 0.5, "H": 0.0}
a_util = {"Yes": -0.8, "No": 0.0}
p_util = lambda p: -0.1 * p

n_peeps = 100
n_tasks = 10
n_alts = 3

def sim_one(id):
    datlist = []

    for t in range(1, n_tasks + 1):
        sampled_profiles = profiles.sample(n=n_alts).copy()
        sampled_profiles["resp"] = id
        sampled_profiles["task"] = t

        sampled_profiles["v"] = sampled_profiles["brand"].map(b_util) + \
                                sampled_profiles["ad"].map(a_util) + \
                                sampled_profiles["price"].apply(p_util)
        sampled_profiles["v"] = sampled_profiles["v"].round(10)
        sampled_profiles["e"] = -np.log(-np.log(np.random.rand(n_alts)))
        sampled_profiles["u"] = sampled_profiles["v"] + sampled_profiles["e"]
        sampled_profiles["choice"] = (sampled_profiles["u"] == sampled_profiles["u"].max()).astype(int)

        datlist.append(sampled_profiles)

    return pd.concat(datlist)

conjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)
conjoint_data = conjoint_data[["resp", "task", "brand", "ad", "price", "choice"]]

print(conjoint_data.head())
```
::::



## 3. Preparing the Data for Estimation

The "hard part" of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer $i$, covariate $k$, and product $j$) instead of the typical 2 dimensions for cross-sectional regression models (consumer $i$ and covariate $k$). The fact that each task for each respondent has the same number of alternatives (3) helps.  In addition, we need to convert the categorical variables for brand and ads into binary variables.

```{python}
# One-hot encode brand and ad
conjoint_data = pd.read_csv('conjoint_data.csv')
df = conjoint_data.copy()
df = pd.get_dummies(df, columns=['brand','ad'], drop_first=False)

# Rename columns for clarity
df.rename(columns={'brand_N':'beta_netflix', 'brand_P':'beta_prime', 'ad_Yes':'beta_ads'}, inplace=True)

df.sort_values(['resp','task'], inplace=True)
df.head()
```


## 4. Estimation via Maximum Likelihood

We define the negative log‐likelihood and optimize with `scipy.optimize.minimize`.  Standard errors come from the inverse Hessian.  We then compute 95% confidence intervals.

```{python}
from scipy.optimize import minimize

# Design matrix and response
X = df[['beta_netflix','beta_prime','beta_ads','price']].to_numpy(dtype=float)
y = df['choice'].to_numpy(dtype=int)
n_alt = 3
n_obs = len(y) // n_alt

# Define negative log‐likelihood
def neg_loglik(beta):
    # reshape X into (n_obs, n_alt, n_cov)
    XB = X.reshape(n_obs, n_alt, -1)          
    utility = XB @ beta                     
    exp_u = np.exp(utility)  
    probs = exp_u / exp_u.sum(axis=1, keepdims=True)
    y_mat = y.reshape(n_obs, n_alt)
    ll = np.sum(y_mat * np.log(probs))
    return -ll

# Optimize with BFGS
init = np.zeros(4, dtype=float)
res = minimize(neg_loglik, init, method='BFGS')

# Extract estimates, standard errors, and CIs
beta_hat = res.x
hess_inv = res.hess_inv             
se = np.sqrt(np.diag(hess_inv))
ci_lower = beta_hat - 1.96*se
ci_upper = beta_hat + 1.96*se

results_mle = pd.DataFrame({
    'Estimate':    beta_hat,
    'Std. Error':  se,
    'CI 2.5%':     ci_lower,
    'CI 97.5%':    ci_upper
}, index=['Netflix','Prime','Ads','Price'])

print(results_mle)
```


## 5. Estimation via Bayesian Methods

Now we'll implement a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the parameters.

```{python}
import matplotlib.pyplot as plt

# Define log‐prior
def log_prior(beta):
    # beta[0:3] ~ N(0,5), beta[3] ~ N(0,1)
    lp = -0.5 * (beta[0]**2/5 + beta[1]**2/5 + beta[2]**2/5 + beta[3]**2/1)
    return lp

# Define log‐posterior (up to constant)
def log_post(beta):
    return -neg_loglik(beta) + log_prior(beta)

# MCMC settings
n_iter = 11000
burn_in = 1000
draws = np.zeros((n_iter, 4))
draws[0] = beta_hat 
scales = np.array([0.05, 0.05, 0.05, 0.005])

# Run Metropolis–Hastings
for t in range(1, n_iter):
    cand = draws[t-1] + np.random.randn(4) * scales
    log_alpha = log_post(cand) - log_post(draws[t-1])
    if np.log(np.random.rand()) < log_alpha:
        draws[t] = cand
    else:
        draws[t] = draws[t-1]

# Discard burn-in
samples = draws[burn_in:]
```

Create trace plots and posterior distributions for Netflix's parameter:

```{python}
# Trace plot & histogram for Netflix beta
plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.plot(samples[:,0], linewidth=0.5)
plt.title('Trace: β_Netflix')
plt.xlabel('Iteration')
plt.ylabel('Value')

plt.subplot(1,2,2)
plt.hist(samples[:,0], bins=30, edgecolor='k')
plt.title('Posterior: β_Netflix')
plt.xlabel('Value')
plt.ylabel('Frequency')

plt.tight_layout()
plt.show()
```

Compare the MLE and Bayesian results:

```{python}
# Summarize posterior for all 4 betas
df_post = pd.DataFrame(samples, columns=['Netflix','Prime','Ads','Price'])
summary_bayes = pd.DataFrame({
    'Mean': df_post.mean(),
    'Std.Dev': df_post.std(),
    '2.5%': df_post.quantile(0.025),
    '97.5%': df_post.quantile(0.975)
})
print(summary_bayes)

```

## 6. Discussion

### Interpretation of Parameter Estimates

Even if you hadn't generated the data yourself, the estimated coefficients still tell you how each attribute drives choice. In both the MLE and Bayesian results:

**βNetflix ≈ 0.94 and βPrime ≈ 0.50**: Thus respondents derive more utility from Netflix than from Amazon Prime. Formally, βNetflix > βPrime means that, all else equal, the probability of choosing Netflix exceeds that of choosing Prime. Both methods yielded nearly identical results (MLE: 0.941 vs Bayesian: 0.930 for Netflix; MLE: 0.502 vs Bayesian: 0.490 for Prime), demonstrating the robustness of both estimation approaches.

**βAds ≈ −0.73** shows that having ads reduces utility relative to an ad-free experience. This substantial negative effect indicates that advertisements significantly diminish the attractiveness of streaming services.

**βPrice ≈ −0.10** is negative, which makes perfect sense: higher monthly cost lowers the attractiveness of a streaming option. Each additional dollar reduces utility by approximately 0.1 units, suggesting consumers are reasonably price-sensitive in this subscription market. The tight confidence/credible intervals around these estimates (none straddling zero) confirm that these effects are both statistically and substantively meaningful.

### Toward a Hierarchical (Random-Parameter) MNL

A multi-level or random-parameter MNL allows each respondent i to have their own taste vector βi, drawn from a population distribution:

$$\beta_i \sim N(\mu, \Sigma)$$

To **simulate** such data, you would first draw each individual's βi from N(μ, Σ), then for each choice task compute utilities and simulate choices using that person-specific βi. 

To **estimate** the hierarchical model you'd:

1. **Introduce hyperpriors** on μ and Σ, specifying prior distributions for both the population means and the variance-covariance matrix.

2. **Implement a Bayesian sampler** (e.g., Gibbs or HMC) that cycles between updating each respondent's βi (conditional on data and μ, Σ) and updating μ, Σ (conditional on all βi). Estimation becomes computationally intensive because the likelihood requires integration over the random effects distribution for each individual.

This extension captures heterogeneity in preferences ("some people love Netflix much more than others") and is the standard approach for real-world conjoint analysis because it enables market segmentation, individual-level predictions, and more realistic uncertainty quantification about consumer behavior.









