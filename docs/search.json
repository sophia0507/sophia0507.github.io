[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header-1",
    "href": "projects/project1/index.html#sub-header-1",
    "title": "Analysis of Cars",
    "section": "Sub-Header",
    "text": "Sub-Header\nHere is a plot:\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt"
  },
  {
    "objectID": "projects/hw4-1a/index.html",
    "href": "projects/hw4-1a/index.html",
    "title": "1a. K-Means",
    "section": "",
    "text": "In this section, I implement the K-Means clustering algorithm from scratch in Python, visualize its iterative process, and compare the results with those from scikit-learn’s built-in implementation. The analysis is based on the Palmer Penguins dataset, using the bill_length_mm and flipper_length_mm features."
  },
  {
    "objectID": "projects/hw4-1a/index.html#introduction",
    "href": "projects/hw4-1a/index.html#introduction",
    "title": "1a. K-Means",
    "section": "",
    "text": "In this section, I implement the K-Means clustering algorithm from scratch in Python, visualize its iterative process, and compare the results with those from scikit-learn’s built-in implementation. The analysis is based on the Palmer Penguins dataset, using the bill_length_mm and flipper_length_mm features."
  },
  {
    "objectID": "projects/hw4-1a/index.html#dataset-preparation",
    "href": "projects/hw4-1a/index.html#dataset-preparation",
    "title": "1a. K-Means",
    "section": "Dataset Preparation",
    "text": "Dataset Preparation\n\nimport pandas as pd\nimport numpy as np\n\n# Load dataset\npenguins = pd.read_csv(\"palmer_penguins.csv\")\n# Select features and drop missing values\npenguins = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\nX = penguins.values\n\n# Quick summary\nprint(penguins.describe())\n\n       bill_length_mm  flipper_length_mm\ncount      333.000000         333.000000\nmean        43.992793         200.966967\nstd          5.468668          14.015765\nmin         32.100000         172.000000\n25%         39.500000         190.000000\n50%         44.500000         197.000000\n75%         48.600000         213.000000\nmax         59.600000         231.000000"
  },
  {
    "objectID": "projects/hw4-1a/index.html#custom-k-means-implementation",
    "href": "projects/hw4-1a/index.html#custom-k-means-implementation",
    "title": "1a. K-Means",
    "section": "Custom K-Means Implementation",
    "text": "Custom K-Means Implementation\nDefine core functions: - euclidean_distance: compute distance between points\n\ninitialize_centroids: random selection of initial centroids\nassign_clusters: assign each point to nearest centroid\nupdate_centroids: recompute centroids\n\n\ndef euclidean_distance(a, b):\n    return np.linalg.norm(a - b, axis=1)\n\ndef initialize_centroids(X, k):\n    idx = np.random.choice(X.shape[0], k, replace=False)\n    return X[idx]\n\ndef assign_clusters(X, centroids):\n    return np.array([np.argmin(euclidean_distance(x, centroids)) for x in X])\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans_custom(X, k, max_iters=100):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return centroids, labels"
  },
  {
    "objectID": "projects/hw4-1a/index.html#visualization-of-algorithm-steps",
    "href": "projects/hw4-1a/index.html#visualization-of-algorithm-steps",
    "title": "1a. K-Means",
    "section": "Visualization of Algorithm Steps",
    "text": "Visualization of Algorithm Steps\nI animate the movement of centroids across iterations for K=3 and save the result as kmeans_steps.gif.\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation\n\nK = 3\ncentroids, labels = kmeans_custom(X, K)\n\nfig, ax = plt.subplots()\nscatter = ax.scatter(X[:,0], X[:,1], c=labels, cmap='viridis', s=50)\ncent_plot = ax.scatter(centroids[:,0], centroids[:,1], c='red', marker='X', s=100)\nax.set_xlabel('Bill Length (mm)')\nax.set_ylabel('Flipper Length (mm)')\nax.set_title('K-Means Iterations (K=3)')\n\ndef update(frame):\n    global centroids, labels\n    labels = assign_clusters(X, centroids)\n    centroids = update_centroids(X, labels, K)\n    scatter.set_array(labels)\n    cent_plot.set_offsets(centroids)\n    return scatter, cent_plot\n\nanim = FuncAnimation(fig, update, frames=10, interval=500, blit=True)\nanim.save('kmeans_steps.gif', writer='pillow')"
  },
  {
    "objectID": "projects/hw4-1a/index.html#metric-comparison",
    "href": "projects/hw4-1a/index.html#metric-comparison",
    "title": "1a. K-Means",
    "section": "Metric Comparison",
    "text": "Metric Comparison\nTo evaluate clustering quality, both the Within-Cluster Sum of Squares (WSS) and the Silhouette Score are computed for values of K ranging from 2 to 7.\n\ndef compute_wss(X, labels, centroids):\n    total = 0.0\n    for i in range(centroids.shape[0]):\n        cluster_pts = X[labels == i]\n        total += np.sum((cluster_pts - centroids[i]) ** 2)\n    return total\n\ndef silhouette_score_custom(X, labels):\n    n = X.shape[0]\n    unique_labels = np.unique(labels)\n    sil_vals = np.zeros(n)\n    for i in range(n):\n        own = labels[i]\n        same = X[labels == own]\n        # a_i: mean distance to points in same cluster\n        a = np.mean(np.linalg.norm(same - X[i], axis=1))\n        # b_i: min mean distance to points in other clusters\n        b = np.min([\n            np.mean(np.linalg.norm(X[labels == other] - X[i], axis=1))\n            for other in unique_labels if other != own\n        ])\n        sil_vals[i] = (b - a) / max(a, b)\n    return np.mean(sil_vals)\n\nwss_list = []\nsil_list = []\nKs = range(2, 8)\n\nfor k in Ks:\n\n    centroids, labels = kmeans_custom(X, k)\n    wss_list.append(compute_wss(X, labels, centroids))\n    sil_list.append(silhouette_score_custom(X, labels))\n\n\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 2, figsize=(12, 4))\n\naxes[0].plot(Ks, wss_list, marker='o')\naxes[0].set_title('WSS vs K')\naxes[0].set_xlabel('Number of Clusters')\naxes[0].set_ylabel('WSS')\n\naxes[1].plot(Ks, sil_list, marker='o')\naxes[1].set_title('Silhouette Score vs K')\naxes[1].set_xlabel('Number of Clusters')\naxes[1].set_ylabel('Silhouette Score')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "projects/hw4-1a/index.html#interpretation",
    "href": "projects/hw4-1a/index.html#interpretation",
    "title": "1a. K-Means",
    "section": "Interpretation",
    "text": "Interpretation\n\nWSS (Within-Cluster Sum of Squares): A sharp decrease in WSS is observed from K = 2 to K = 3, followed by a more gradual decline for larger K values. This creates a noticeable “elbow” at K = 3, which typically suggests an optimal number of clusters—beyond which additional clusters contribute little to reducing within-group variance.\nSilhouette Score: The highest silhouette score appears at K = 2, indicating the strongest separation between clusters. However, the score remains relatively stable from K = 3 to K = 5, with only a slight drop at K = 3, suggesting that cluster cohesion and separation are still well maintained."
  },
  {
    "objectID": "projects/hw4-1a/index.html#conclusion",
    "href": "projects/hw4-1a/index.html#conclusion",
    "title": "1a. K-Means",
    "section": "Conclusion",
    "text": "Conclusion\nAlthough K = 2 has the highest silhouette score, the WSS plot strongly supports K = 3 as the elbow point. Considering both metrics, K = 3 provides a solid compromise between model simplicity and clustering quality, making it a well-balanced choice for this dataset."
  },
  {
    "objectID": "projects/hw2/index.html",
    "href": "projects/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import factorial\nimport scipy.stats\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\nimport scipy.optimize as sp\nimport scipy.stats as stats\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import minimize_scalar,minimize, approx_fprime\n\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Map numeric to label\ndf[\"customer_label\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Create histogram\nax = sns.histplot(data=df, x=\"patents\", hue=\"customer_label\", multiple=\"dodge\", \n                  bins=30, palette={\"Non-Customer\": \"orange\", \"Customer\": \"skyblue\"})\n\n# Add title and axis labels\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by customer status\ndf.groupby(\"customer_label\")[\"patents\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_label\nmean\nstd\ncount\n\n\n\n\n0\nCustomer\n4.133056\n2.546846\n481\n\n\n1\nNon-Customer\n3.473013\n2.225060\n1019\n\n\n\n\n\n\n\nThe histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count.\nThe summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution\nregion_ct = pd.crosstab(df[\"region\"], df[\"customer_label\"], normalize=\"columns\") * 100\nregion_ct.columns= [\"Customer (%)\", \"Non-Customer (%)\"]\nregion_ct.index.name = \"Region\"\nregion_ct = region_ct.reset_index()\nprint(region_ct.to_string(index=False))\n\n   Region  Customer (%)  Non-Customer (%)\n  Midwest      7.692308         18.351325\nNortheast     68.191268         26.790972\nNorthwest      6.029106         15.505397\n    South      7.276507         15.309127\nSouthwest     10.810811         24.043180\n\n\n\n# Age distribution\nage_summary = df.groupby(\"customer_label\")[\"age\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(2)\nage_summary = age_summary.reset_index()\nage_summary.columns = [\"Customer Status\", \"Mean Age\", \"Std Dev\", \"Min Age\", \"Max Age\"]\nage_summary\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\nStd Dev\nMin Age\nMax Age\n\n\n\n\n0\nCustomer\n26.9\n7.81\n10.0\n49.0\n\n\n1\nNon-Customer\n26.1\n6.95\n9.0\n47.5\n\n\n\n\n\n\n\n\nages = np.linspace(df[\"age\"].min(), df[\"age\"].max(), 300)\nkde_non = gaussian_kde(df.loc[df[\"customer_label\"]==\"Non-Customer\",\"age\"])(ages)\nkde_cust = gaussian_kde(df.loc[df[\"customer_label\"]==\"Customer\",\"age\"])(ages)\n\nplt.plot(ages, kde_non, label=\"Non-Customer\", color=\"tab:orange\")\nplt.plot(ages, kde_cust, label=\"Customer\",    color=\"tab:blue\")\nplt.fill_between(ages, kde_non, alpha=0.3, color=\"tab:orange\")\nplt.fill_between(ages, kde_cust, alpha=0.3, color=\"tab:blue\")\n\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nA review of the regional distribution shows that Blueprinty customers are not evenly distributed across the United States. In particular, the Northeast region accounts for the majority of customers (68.2%), while only 26.7% of non-customers are located there. In contrast, regions like the Midwest, South, and Southwest are relatively underrepresented among customers compared to non-customers. This suggests strong geographic clustering in Blueprinty’s user base, potentially reflecting targeted marketing or regional technology adoption patterns.\nIn terms of firm age, the average age of customers is slightly younger at 26.1 years (std = 6.95), compared to 26.9 years (std = 7.81) for non-customers. Although the means are close, the density plot further reveals that non-customers have a higher peak in the 24–26 age range and a more pronounced tail toward older firms (ages 35+), while customers are more concentrated around the mid-20s.\nThese observed differences in both region and age imply that Blueprinty customers may differ systematically from non-customers. Consequently, it’s important to include these covariates in the regression model to avoid attributing differences in patent counts solely to software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe now define the log-likelihood function for the Poisson model::\n\ndef poisson_loglikelihood(lambda_val, Y):\n    return -lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1)\n\ndef poisson_loglikelihood_sum(lambda_val, Y):\n    return np.sum(-lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1))\n\nNow let’s plot the log-likelihood for a range of lambda values:\n\n# Get the observed patents data\nobserved_patents = df['patents'].values\n\n# Choose a reasonable range of lambda values based on data\nlambda_range = np.linspace(0.1, 20, 100)\nll_values = np.zeros(len(lambda_range))\n\n# Calculate log-likelihood for each lambda\nfor i, lam in enumerate(lambda_range):\n    ll_values[i] = np.sum([poisson_loglikelihood(lam, y) for y in observed_patents])\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, ll_values)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Model for Different Lambda Values')\nplt.axvline(x=observed_patents.mean(), color='red', linestyle='--', \n            label=f'Sample Mean (λ_MLE = {observed_patents.mean():.2f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) for lambda in a Poisson distribution is the sample mean. Mathematically, we can show this by taking the derivative of the log-likelihood function:\n\\(\\frac{d}{d\\lambda} \\ln f(Y|\\lambda) = -1 + \\frac{Y}{\\lambda} = 0\\)\nSolving for lambda: \\(\\lambda_{MLE} = Y\\)\nFor a sample of observations, the MLE is the sample mean: \\(\\lambda_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\\)\nThe following optimization step confirms the MLE matches the sample mean:\n\n# Define the negative log-likelihood (for minimization)\ndef neg_poisson_loglikelihood_sum(lambda_val, Y):\n    return -poisson_loglikelihood_sum(lambda_val, Y)\n\n# Find the MLE using optimization\ninitial_guess = 1.0  # Starting point for optimization\nresult = minimize(neg_poisson_loglikelihood_sum, \n                  x0=[1.0], \n                  args=(observed_patents,), \n                  method='L-BFGS-B',\n                  bounds=[(1e-6, None)])\n\nprint(f\"MLE estimate for lambda: {result.x[0]:.4f}\")\nprint(f\"Sample mean: {observed_patents.mean():.4f}\")\nprint(f\"Are they equal? {np.isclose(result.x[0], observed_patents.mean())}\")\n\nMLE estimate for lambda: 3.6847\nSample mean: 3.6847\nAre they equal? True\n\n\nAs shown in the plot and confirmed by optimization, the maximum of the log-likelihood function occurs at λ = 3.6847, which is exactly the sample mean. This aligns with the known analytical result for the MLE of λ in a Poisson model. The red dashed line highlights the MLE and validates our implementation of the likelihood-based estimation approach.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by defining the log-likelihood function for the Poisson regression model:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.array(beta, dtype=np.float64)\n    Y = np.array(Y, dtype=np.float64)\n    X = np.array(X, dtype=np.float64)\n    linear_pred = np.dot(X, beta)\n    lambda_i = np.exp(np.clip(linear_pred, -30, 30))\n    ll_components = -lambda_i + Y * np.log(lambda_i)\n    return -np.sum(ll_components)\n\n\n\n\n\n# Add squared age and region dummies\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Combine into design matrix\nX_data = pd.concat([\n    pd.DataFrame({'intercept': 1}, index=df.index),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX = X_data.values.astype(np.float64)\ny = df['patents'].values.astype(np.float64)\n\n\n\n\n\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\ninitial_beta = glm_results.params\n\n\n\n\n\n\nresult_poisson_reg = minimize(\n    poisson_regression_loglikelihood,\n    initial_beta,\n    args=(y, X),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_estimates = result_poisson_reg.x\n\n         Current function value: -1790.814845\n         Iterations: 0\n         Function evaluations: 264\n         Gradient evaluations: 28\n\n\nE:\\2025 Spring - ucsd\\Sophia's Website\\quarto-env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\n\n\n\n\n\ndef hessian(func, x, epsilon=1e-5):\n    n = len(x)\n    hess = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            x1, x2, x3, x4 = x.copy(), x.copy(), x.copy(), x.copy()\n            if i == j:\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                hess[i, i] = (func(x1) + func(x2) - 2 * func(x)) / (epsilon ** 2)\n            else:\n                x1[i] += epsilon; x1[j] += epsilon\n                x2[i] += epsilon; x2[j] -= epsilon\n                x3[i] -= epsilon; x3[j] += epsilon\n                x4[i] -= epsilon; x4[j] -= epsilon\n                hess[i, j] = (func(x1) + func(x4) - func(x2) - func(x3)) / (4 * epsilon ** 2)\n                hess[j, i] = hess[i, j]\n    return hess\n\nH = hessian(lambda b: poisson_regression_loglikelihood(b, y, X), beta_estimates)\ncov_matrix = np.linalg.inv(H)\nstd_errors = np.sqrt(np.diag(cov_matrix))\n\nz_values = beta_estimates / std_errors\np_values = 2 * (1 - np.abs(scipy.stats.norm.cdf(z_values)))\n\ncustom_results = pd.DataFrame({\n    'Coefficient': beta_estimates,\n    'Std. Error': std_errors,\n    'z-value': z_values,\n    'p-value': p_values\n}, index=X_data.columns)\n\ncustom_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1837\n-2.7698\n1.9944\n\n\nage\n0.1486\n0.0139\n10.6800\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.4738\n2.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7190\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n1.2562\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\n\n\n\n\nsm_results = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z-value': glm_results.tvalues,\n    'p-value': glm_results.pvalues\n}, index=X_data.columns)\n\nsm_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe estimates and standard errors obtained using scipy.optimize match those from statsmodels, confirming the correctness of our custom likelihood implementation.\n\n\n\nThe model successfully estimated the impact of several firm-level characteristics on patent success. we found:\n\nFirm Age: A positive and significant effect (age coefficient = 0.1486, p &lt; 0.001), with a small negative effect from age squared. This suggests diminishing returns to age: patent counts increase with age but at a decreasing rate.\nRegion: None of the region coefficients were statistically significant at the 5% level, indicating little evidence of regional differences in patent activity once other variables are controlled for.\nBlueprinty Software (iscustomer): The coefficient for iscustomer is 0.2076 with a p-value of &lt; 0.001, meaning it is both positive and highly statistically significant.\n\n\n\n\n\nTo interpret the effect of the binary variable iscustomer, we simulate two scenarios: - (X_0): all firms are non-customers (iscustomer = 0) - (X_1): all firms are customers (iscustomer = 1)\nwe compare the average predicted number of patents under each scenario:\n\nX_0 = X.copy()\nX_1 = X.copy()\niscustomer_idx = list(X_data.columns).index('iscustomer')\nX_0[:, iscustomer_idx] = 0\nX_1[:, iscustomer_idx] = 1\n\nlambda_0 = np.exp(np.dot(X_0, beta_estimates))\nlambda_1 = np.exp(np.dot(X_1, beta_estimates))\n\ndifferences = lambda_1 - lambda_0\navg_effect = np.mean(differences)\nperc_increase = 100 * avg_effect / np.mean(lambda_0)\n\nprint(\"Average predicted increase in patents:\", round(avg_effect, 3), \"per firm\")\nprint(\"Percent increase:\", round(perc_increase, 2), \"%\")\n\nAverage predicted increase in patents: 0.793 per firm\nPercent increase: 23.07 %\n\n\nThis means that, holding all else constant, firms using Blueprinty’s software are predicted to earn about 0.79 more patents over 5 years, which represents a 23% improvement compared to non-customers.\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(differences, bins=30, alpha=0.7)\nplt.axvline(avg_effect, color='red', linestyle='--', \n            label=f'Avg Effect: {avg_effect:.2f} patents')\nplt.xlabel('Increase in Predicted Patents')\nplt.ylabel('Number of Firms')\nplt.title('Effect of Using Blueprinty Software')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that nearly all firms are expected to benefit from adopting Blueprinty’s software, reinforcing the interpretation that the product positively contributes to patenting success.\nThis result supports the company’s marketing claim, though causal inference would require further analysis beyond this observational model."
  },
  {
    "objectID": "projects/hw2/index.html#blueprinty-case-study",
    "href": "projects/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import factorial\nimport scipy.stats\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\nimport scipy.optimize as sp\nimport scipy.stats as stats\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import minimize_scalar,minimize, approx_fprime\n\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Map numeric to label\ndf[\"customer_label\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Create histogram\nax = sns.histplot(data=df, x=\"patents\", hue=\"customer_label\", multiple=\"dodge\", \n                  bins=30, palette={\"Non-Customer\": \"orange\", \"Customer\": \"skyblue\"})\n\n# Add title and axis labels\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by customer status\ndf.groupby(\"customer_label\")[\"patents\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_label\nmean\nstd\ncount\n\n\n\n\n0\nCustomer\n4.133056\n2.546846\n481\n\n\n1\nNon-Customer\n3.473013\n2.225060\n1019\n\n\n\n\n\n\n\nThe histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count.\nThe summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution\nregion_ct = pd.crosstab(df[\"region\"], df[\"customer_label\"], normalize=\"columns\") * 100\nregion_ct.columns= [\"Customer (%)\", \"Non-Customer (%)\"]\nregion_ct.index.name = \"Region\"\nregion_ct = region_ct.reset_index()\nprint(region_ct.to_string(index=False))\n\n   Region  Customer (%)  Non-Customer (%)\n  Midwest      7.692308         18.351325\nNortheast     68.191268         26.790972\nNorthwest      6.029106         15.505397\n    South      7.276507         15.309127\nSouthwest     10.810811         24.043180\n\n\n\n# Age distribution\nage_summary = df.groupby(\"customer_label\")[\"age\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(2)\nage_summary = age_summary.reset_index()\nage_summary.columns = [\"Customer Status\", \"Mean Age\", \"Std Dev\", \"Min Age\", \"Max Age\"]\nage_summary\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\nStd Dev\nMin Age\nMax Age\n\n\n\n\n0\nCustomer\n26.9\n7.81\n10.0\n49.0\n\n\n1\nNon-Customer\n26.1\n6.95\n9.0\n47.5\n\n\n\n\n\n\n\n\nages = np.linspace(df[\"age\"].min(), df[\"age\"].max(), 300)\nkde_non = gaussian_kde(df.loc[df[\"customer_label\"]==\"Non-Customer\",\"age\"])(ages)\nkde_cust = gaussian_kde(df.loc[df[\"customer_label\"]==\"Customer\",\"age\"])(ages)\n\nplt.plot(ages, kde_non, label=\"Non-Customer\", color=\"tab:orange\")\nplt.plot(ages, kde_cust, label=\"Customer\",    color=\"tab:blue\")\nplt.fill_between(ages, kde_non, alpha=0.3, color=\"tab:orange\")\nplt.fill_between(ages, kde_cust, alpha=0.3, color=\"tab:blue\")\n\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nA review of the regional distribution shows that Blueprinty customers are not evenly distributed across the United States. In particular, the Northeast region accounts for the majority of customers (68.2%), while only 26.7% of non-customers are located there. In contrast, regions like the Midwest, South, and Southwest are relatively underrepresented among customers compared to non-customers. This suggests strong geographic clustering in Blueprinty’s user base, potentially reflecting targeted marketing or regional technology adoption patterns.\nIn terms of firm age, the average age of customers is slightly younger at 26.1 years (std = 6.95), compared to 26.9 years (std = 7.81) for non-customers. Although the means are close, the density plot further reveals that non-customers have a higher peak in the 24–26 age range and a more pronounced tail toward older firms (ages 35+), while customers are more concentrated around the mid-20s.\nThese observed differences in both region and age imply that Blueprinty customers may differ systematically from non-customers. Consequently, it’s important to include these covariates in the regression model to avoid attributing differences in patent counts solely to software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe now define the log-likelihood function for the Poisson model::\n\ndef poisson_loglikelihood(lambda_val, Y):\n    return -lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1)\n\ndef poisson_loglikelihood_sum(lambda_val, Y):\n    return np.sum(-lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1))\n\nNow let’s plot the log-likelihood for a range of lambda values:\n\n# Get the observed patents data\nobserved_patents = df['patents'].values\n\n# Choose a reasonable range of lambda values based on data\nlambda_range = np.linspace(0.1, 20, 100)\nll_values = np.zeros(len(lambda_range))\n\n# Calculate log-likelihood for each lambda\nfor i, lam in enumerate(lambda_range):\n    ll_values[i] = np.sum([poisson_loglikelihood(lam, y) for y in observed_patents])\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, ll_values)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Model for Different Lambda Values')\nplt.axvline(x=observed_patents.mean(), color='red', linestyle='--', \n            label=f'Sample Mean (λ_MLE = {observed_patents.mean():.2f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) for lambda in a Poisson distribution is the sample mean. Mathematically, we can show this by taking the derivative of the log-likelihood function:\n\\(\\frac{d}{d\\lambda} \\ln f(Y|\\lambda) = -1 + \\frac{Y}{\\lambda} = 0\\)\nSolving for lambda: \\(\\lambda_{MLE} = Y\\)\nFor a sample of observations, the MLE is the sample mean: \\(\\lambda_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\\)\nThe following optimization step confirms the MLE matches the sample mean:\n\n# Define the negative log-likelihood (for minimization)\ndef neg_poisson_loglikelihood_sum(lambda_val, Y):\n    return -poisson_loglikelihood_sum(lambda_val, Y)\n\n# Find the MLE using optimization\ninitial_guess = 1.0  # Starting point for optimization\nresult = minimize(neg_poisson_loglikelihood_sum, \n                  x0=[1.0], \n                  args=(observed_patents,), \n                  method='L-BFGS-B',\n                  bounds=[(1e-6, None)])\n\nprint(f\"MLE estimate for lambda: {result.x[0]:.4f}\")\nprint(f\"Sample mean: {observed_patents.mean():.4f}\")\nprint(f\"Are they equal? {np.isclose(result.x[0], observed_patents.mean())}\")\n\nMLE estimate for lambda: 3.6847\nSample mean: 3.6847\nAre they equal? True\n\n\nAs shown in the plot and confirmed by optimization, the maximum of the log-likelihood function occurs at λ = 3.6847, which is exactly the sample mean. This aligns with the known analytical result for the MLE of λ in a Poisson model. The red dashed line highlights the MLE and validates our implementation of the likelihood-based estimation approach.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by defining the log-likelihood function for the Poisson regression model:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.array(beta, dtype=np.float64)\n    Y = np.array(Y, dtype=np.float64)\n    X = np.array(X, dtype=np.float64)\n    linear_pred = np.dot(X, beta)\n    lambda_i = np.exp(np.clip(linear_pred, -30, 30))\n    ll_components = -lambda_i + Y * np.log(lambda_i)\n    return -np.sum(ll_components)\n\n\n\n\n\n# Add squared age and region dummies\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Combine into design matrix\nX_data = pd.concat([\n    pd.DataFrame({'intercept': 1}, index=df.index),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX = X_data.values.astype(np.float64)\ny = df['patents'].values.astype(np.float64)\n\n\n\n\n\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\ninitial_beta = glm_results.params\n\n\n\n\n\n\nresult_poisson_reg = minimize(\n    poisson_regression_loglikelihood,\n    initial_beta,\n    args=(y, X),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_estimates = result_poisson_reg.x\n\n         Current function value: -1790.814845\n         Iterations: 0\n         Function evaluations: 264\n         Gradient evaluations: 28\n\n\nE:\\2025 Spring - ucsd\\Sophia's Website\\quarto-env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\n\n\n\n\n\ndef hessian(func, x, epsilon=1e-5):\n    n = len(x)\n    hess = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            x1, x2, x3, x4 = x.copy(), x.copy(), x.copy(), x.copy()\n            if i == j:\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                hess[i, i] = (func(x1) + func(x2) - 2 * func(x)) / (epsilon ** 2)\n            else:\n                x1[i] += epsilon; x1[j] += epsilon\n                x2[i] += epsilon; x2[j] -= epsilon\n                x3[i] -= epsilon; x3[j] += epsilon\n                x4[i] -= epsilon; x4[j] -= epsilon\n                hess[i, j] = (func(x1) + func(x4) - func(x2) - func(x3)) / (4 * epsilon ** 2)\n                hess[j, i] = hess[i, j]\n    return hess\n\nH = hessian(lambda b: poisson_regression_loglikelihood(b, y, X), beta_estimates)\ncov_matrix = np.linalg.inv(H)\nstd_errors = np.sqrt(np.diag(cov_matrix))\n\nz_values = beta_estimates / std_errors\np_values = 2 * (1 - np.abs(scipy.stats.norm.cdf(z_values)))\n\ncustom_results = pd.DataFrame({\n    'Coefficient': beta_estimates,\n    'Std. Error': std_errors,\n    'z-value': z_values,\n    'p-value': p_values\n}, index=X_data.columns)\n\ncustom_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1837\n-2.7698\n1.9944\n\n\nage\n0.1486\n0.0139\n10.6800\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.4738\n2.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7190\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n1.2562\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\n\n\n\n\nsm_results = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z-value': glm_results.tvalues,\n    'p-value': glm_results.pvalues\n}, index=X_data.columns)\n\nsm_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe estimates and standard errors obtained using scipy.optimize match those from statsmodels, confirming the correctness of our custom likelihood implementation.\n\n\n\nThe model successfully estimated the impact of several firm-level characteristics on patent success. we found:\n\nFirm Age: A positive and significant effect (age coefficient = 0.1486, p &lt; 0.001), with a small negative effect from age squared. This suggests diminishing returns to age: patent counts increase with age but at a decreasing rate.\nRegion: None of the region coefficients were statistically significant at the 5% level, indicating little evidence of regional differences in patent activity once other variables are controlled for.\nBlueprinty Software (iscustomer): The coefficient for iscustomer is 0.2076 with a p-value of &lt; 0.001, meaning it is both positive and highly statistically significant.\n\n\n\n\n\nTo interpret the effect of the binary variable iscustomer, we simulate two scenarios: - (X_0): all firms are non-customers (iscustomer = 0) - (X_1): all firms are customers (iscustomer = 1)\nwe compare the average predicted number of patents under each scenario:\n\nX_0 = X.copy()\nX_1 = X.copy()\niscustomer_idx = list(X_data.columns).index('iscustomer')\nX_0[:, iscustomer_idx] = 0\nX_1[:, iscustomer_idx] = 1\n\nlambda_0 = np.exp(np.dot(X_0, beta_estimates))\nlambda_1 = np.exp(np.dot(X_1, beta_estimates))\n\ndifferences = lambda_1 - lambda_0\navg_effect = np.mean(differences)\nperc_increase = 100 * avg_effect / np.mean(lambda_0)\n\nprint(\"Average predicted increase in patents:\", round(avg_effect, 3), \"per firm\")\nprint(\"Percent increase:\", round(perc_increase, 2), \"%\")\n\nAverage predicted increase in patents: 0.793 per firm\nPercent increase: 23.07 %\n\n\nThis means that, holding all else constant, firms using Blueprinty’s software are predicted to earn about 0.79 more patents over 5 years, which represents a 23% improvement compared to non-customers.\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(differences, bins=30, alpha=0.7)\nplt.axvline(avg_effect, color='red', linestyle='--', \n            label=f'Avg Effect: {avg_effect:.2f} patents')\nplt.xlabel('Increase in Predicted Patents')\nplt.ylabel('Number of Firms')\nplt.title('Effect of Using Blueprinty Software')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that nearly all firms are expected to benefit from adopting Blueprinty’s software, reinforcing the interpretation that the product positively contributes to patenting success.\nThis result supports the company’s marketing claim, though causal inference would require further analysis beyond this observational model."
  },
  {
    "objectID": "projects/hw2/index.html#airbnb-case-study",
    "href": "projects/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Analysis\n\n# Load the AirBnB data\nairbnb_df = pd.read_csv('airbnb.csv',index_col=0)\n\n# Display the first few rows\nairbnb_df.head()\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Check the data structure and missing values\nairbnb_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 40628 entries, 1 to 40628\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   id                         40628 non-null  int64  \n 1   days                       40628 non-null  int64  \n 2   last_scraped               40628 non-null  object \n 3   host_since                 40593 non-null  object \n 4   room_type                  40628 non-null  object \n 5   bathrooms                  40468 non-null  float64\n 6   bedrooms                   40552 non-null  float64\n 7   price                      40628 non-null  int64  \n 8   number_of_reviews          40628 non-null  int64  \n 9   review_scores_cleanliness  30433 non-null  float64\n 10  review_scores_location     30374 non-null  float64\n 11  review_scores_value        30372 non-null  float64\n 12  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(4), object(4)\nmemory usage: 4.3+ MB\n\n\n\n# Summary statistics\nairbnb_df.describe()\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\nData Cleaning and Feature Engineering\n\n# Select relevant columns\nmodel_cols = ['room_type', 'bathrooms',\n    'bedrooms','price' ,'number_of_reviews',  \n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable', \n]\n\n# Drop missing values\nairbnb_clean = airbnb_df[model_cols].dropna()\n\n# Convert categorical variables into dummy variables\nairbnb_clean = pd.get_dummies(airbnb_clean, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\nairbnb_clean.head()\n\n\n\n\n\n\n\n\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\nroom_type_Private room\nroom_type_Shared room\ninstant_bookable_t\n\n\n\n\n1\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nTrue\nFalse\nFalse\n\n\n2\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nFalse\nFalse\nFalse\n\n\n4\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nFalse\nFalse\nFalse\n\n\n6\n1.0\n1.0\n212\n60\n9.0\n9.0\n9.0\nFalse\nFalse\nFalse\n\n\n7\n1.0\n2.0\n250\n60\n10.0\n9.0\n10.0\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n\nFit Poisson Regression Model\n\n# Define target and features\ny = airbnb_clean[\"number_of_reviews\"]\nX = airbnb_clean.drop(columns=[\"number_of_reviews\"])\nX = sm.add_constant(X)  # Add intercept\n\nX = X.astype(float)\ny = y.astype(float)\n\n# Fit Poisson regression\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nresults.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30150\n\n\nModel Family:\nPoisson\nDf Model:\n9\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2900e+05\n\n\nDate:\nTue, 10 Jun 2025\nDeviance:\n9.3653e+05\n\n\nTime:\n16:46:51\nPearson chi2:\n1.41e+06\n\n\nNo. Iterations:\n6\nPseudo R-squ. (CS):\n0.5649\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.5725\n0.016\n223.215\n0.000\n3.541\n3.604\n\n\nbathrooms\n-0.1240\n0.004\n-33.091\n0.000\n-0.131\n-0.117\n\n\nbedrooms\n0.0749\n0.002\n37.698\n0.000\n0.071\n0.079\n\n\nprice\n-1.435e-05\n8.3e-06\n-1.729\n0.084\n-3.06e-05\n1.92e-06\n\n\nreview_scores_cleanliness\n0.1132\n0.001\n75.820\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0768\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0915\n0.002\n-50.902\n0.000\n-0.095\n-0.088\n\n\nroom_type_Private room\n-0.0145\n0.003\n-5.310\n0.000\n-0.020\n-0.009\n\n\nroom_type_Shared room\n-0.2519\n0.009\n-29.229\n0.000\n-0.269\n-0.235\n\n\ninstant_bookable_t\n0.3344\n0.003\n115.748\n0.000\n0.329\n0.340\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nresults_table = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse,\n    \"z-value\": results.tvalues,\n    \"p-value\": results.pvalues\n})\nresults_table.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n3.5725\n0.0160\n223.2145\n0.0000\n\n\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n\n\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n\n\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n\n\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n\n\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n\n\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n\n\nroom_type_Private room\n-0.0145\n0.0027\n-5.3104\n0.0000\n\n\nroom_type_Shared room\n-0.2519\n0.0086\n-29.2286\n0.0000\n\n\ninstant_bookable_t\n0.3344\n0.0029\n115.7477\n0.0000\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nThe number of reviews (used as a proxy for bookings) varies significantly across listing features:\n\nBedrooms (+): Listings with more bedrooms receive more reviews, likely due to larger group bookings.\nBathrooms (-): Surprisingly, more bathrooms are associated with fewer reviews, possibly due to lower turnover (longer stays).\nPrice (n.s.): The price effect is small and not statistically significant.\nReview Scores:\n\nCleanliness (+): Strong positive effect—cleaner listings attract more reviews.\nLocation & Value (-): Both are negatively associated with review counts, possibly due to selection or expectation effects.\n\nRoom Type:\n\nPrivate room (-) and Shared room (−−): Receive fewer reviews than entire homes.\n\nInstant Bookable (+): Listings that allow instant booking get significantly more reviews, likely due to ease of reservation.\n\nThese results suggest that amenities, guest experience, and booking convenience all influence booking volume on Airbnb."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ting-Yu(Sophia) Wang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects/hw1/index.html",
    "href": "projects/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. I assess the balance of the experimental randomization, compare donation rates (and donation amounts), and finally use simulation to illustrate the Law of Large Numbers and the Central Limit Theorem. The following sections detail our approach, findings, and conclusions."
  },
  {
    "objectID": "projects/hw1/index.html#introduction",
    "href": "projects/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. I assess the balance of the experimental randomization, compare donation rates (and donation amounts), and finally use simulation to illustrate the Law of Large Numbers and the Central Limit Theorem. The following sections detail our approach, findings, and conclusions."
  },
  {
    "objectID": "projects/hw1/index.html#data",
    "href": "projects/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nprint(\"Data Info:\")\nprint(df.info())\n\nprint(\"\\nFirst 5 Rows:\")\nprint(df.head())\n\nprint(\"\\nDescriptive Statistics:\")\nprint(df.describe())\n\nData Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\nFirst 5 Rows:\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\nDescriptive Statistics:\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ncontrol_mrm2 = df.loc[df[\"treatment\"] == 0, \"mrm2\"]\ntreatment_mrm2 = df.loc[df[\"treatment\"] == 1, \"mrm2\"]\n\n# t-test\nt_stat, p_val = stats.ttest_ind(treatment_mrm2, control_mrm2, nan_policy=\"omit\")\nprint(\"T-test for mrm2 (months since last donation) by treatment group\")\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n# linear regression of mrm2 on treatment\nreg_balance = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nprint(\"\\nLinear Regression Output (mrm2 ~ treatment):\")\nprint(reg_balance.summary().tables[1])\n\nT-test for mrm2 (months since last donation) by treatment group\nt-statistic: 0.1195, p-value: 0.9049\n\nLinear Regression Output (mrm2 ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n\nInterpretation: Both the t-test and regression show no significant difference in months since last donation between treatment and control groups (p approximately 0.905). This suggests the randomization was successful, with balanced baseline characteristics. Table 1 in the original paper serves this same purpose, to demonstrate that treatment and control groups were comparable before the intervention."
  },
  {
    "objectID": "projects/hw1/index.html#experimental-results",
    "href": "projects/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n1. Barplot: Proportion Donated by Group\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# donation rates\nprop_treatment = df.loc[df[\"treatment\"] == 1, \"gave\"].mean()\nprop_control = df.loc[df[\"treatment\"] == 0, \"gave\"].mean()\n\n# plot\ngroups = [\"Control\", \"Treatment\"]\nprops = [prop_control, prop_treatment]\n\nplt.figure()\nbars = plt.bar(groups, props)\nplt.title(\"Proportion of Donation by Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Proportion Donated\")\nplt.ylim([0, max(props)*1.3])\n\nfor bar, prop in zip(bars, props):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n             f\"{prop:.3%}\", ha='center', va='bottom', fontsize=10)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2. T-test and Bivariate Linear Regression\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n# t-test\ncontrol_gave = df.loc[df[\"treatment\"] == 0, \"gave\"]\ntreatment_gave = df.loc[df[\"treatment\"] == 1, \"gave\"]\n\nt_stat_gave, p_val_gave = stats.ttest_ind(treatment_gave, control_gave, nan_policy=\"omit\")\nprint(\"T-test: Did treatment increase giving?\")\nprint(f\"t-statistic = {t_stat_gave:.4f}, p-value = {p_val_gave:.4f}\")\n\n# linear regression: gave ~ treatment\nreg_gave = smf.ols(\"gave ~ treatment\", data=df).fit()\nprint(\"\\nBivariate Regression Output (gave ~ treatment):\")\nprint(reg_gave.summary().tables[1])\n\nT-test: Did treatment increase giving?\nt-statistic = 3.1014, p-value = 0.0019\n\nBivariate Regression Output (gave ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nInterpretation: The t-test and bivariate regression both show that individuals who received a matching donation offer were more likely to donate. The estimated increase in donation likelihood is approximately 0.4 percentage points and statistically significant. This result directly matches the response rates reported in Table 2A Panel A of the original study: 1.8 percent for the control group and 2.2 percent for the treatment group. These findings suggest that even small framing differences in fundraising appeals, like offering to match donations, can meaningfully affect people’s willingness to give.\n\n\n\n3. Probit Regression\n\nimport statsmodels.formula.api as smf\n\n# probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nprint(\"Probit Regression Output (gave ~ treatment):\")\nprint(probit_model.summary().tables[1])\n\nProbit Regression Output (gave ~ treatment):\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nInterpretation: The coefficient on the treatment variable in the probit model is positive and statistically significant (p-value = 0.002). While the raw coefficient (0.0868) differs from the marginal effect reported in Table 3 of the paper (0.004), this is expected because probit models in statsmodels report coefficients on the latent index rather than marginal effects. Thus, our results successfully replicate Table 3, Column 1 in terms of direction, significance, and inference.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n1. Create ratio1 Dummy and Summary of Donation Rates\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Create dummy for 1:1 ratio group\ndf[\"ratio1\"] = 0\ndf.loc[(df[\"treatment\"] == 1) & (df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0), \"ratio1\"] = 1\n\n# Show proportions\nprint(\"Proportion donating in each match ratio group:\")\nprint(\"1:1 match:\", df.loc[df[\"ratio1\"] == 1, \"gave\"].mean())\nprint(\"2:1 match:\", df.loc[df[\"ratio2\"] == 1, \"gave\"].mean())\nprint(\"3:1 match:\", df.loc[df[\"ratio3\"] == 1, \"gave\"].mean())\n\nProportion donating in each match ratio group:\n1:1 match: 0.020749124225276205\n2:1 match: 0.0226333752469912\n3:1 match: 0.022733399227244138\n\n\n\n\n\n2. T-Tests: Compare Donation Rates Between Match Ratios\n\nfrom scipy import stats\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf[\"ratio1\"] = 0\ndf.loc[(df[\"treatment\"] == 1) & (df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0), \"ratio1\"] = 1\n\n# Extract gave data\ngave_ratio1 = df.loc[df[\"ratio1\"] == 1, \"gave\"]\ngave_ratio2 = df.loc[df[\"ratio2\"] == 1, \"gave\"]\ngave_ratio3 = df.loc[df[\"ratio3\"] == 1, \"gave\"]\n\n# 1:1 vs 2:1\nt_stat_12, p_val_12 = stats.ttest_ind(gave_ratio1, gave_ratio2, nan_policy=\"omit\")\nprint(\"t-test (1:1 vs 2:1):\")\nprint(f\"t-statistic = {t_stat_12:.4f}, p-value = {p_val_12:.4f}\")\n\n# 2:1 vs 3:1\nt_stat_23, p_val_23 = stats.ttest_ind(gave_ratio2, gave_ratio3, nan_policy=\"omit\")\nprint(\"\\nt-test (2:1 vs 3:1):\")\nprint(f\"t-statistic = {t_stat_23:.4f}, p-value = {p_val_23:.4f}\")\n\nt-test (1:1 vs 2:1):\nt-statistic = -0.9650, p-value = 0.3345\n\nt-test (2:1 vs 3:1):\nt-statistic = -0.0501, p-value = 0.9600\n\n\nInterpretation: The t-tests comparing donation rates between match ratios show no statistically significant differences. The comparison between the 1 : 1 and 2 : 1 match groups has a p-value of 0.3345, and between the 2 : 1 and 3 : 1 groups, the p-value is 0.9600. These high p-values suggest that increasing the match ratio does not significantly increase the likelihood of donating. This supports the authors’ note that figures suggest the size of the match ratio has little impact on donor behavior.\n\n\n\n3. Regression: Donation ~ Match Ratio Group (OLS)\n\nimport statsmodels.formula.api as smf\n\nreg_match = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3\", data=df).fit()\nprint(\"Regression Output (gave ~ ratio1 + ratio2 + ratio3):\")\nprint(reg_match.summary().tables[1])\n\nRegression Output (gave ~ ratio1 + ratio2 + ratio3):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\n\n\nInterpretation: The regression results show that the 2 : 1 and 3 : 1 match groups are associated with small but statistically significant increases in donation probability compared to the control group, with p-values below 0.01. The 1 : 1 group is not statistically significant. However, all coefficients are very small in magnitude, with increases less than 0.005. This suggests that although some match sizes show statistically significant differences, the practical effect is minimal. The treatment likely matters more than the exact size of the match.\n\n\n\n4. Direct Comparison of Response Rates\n\nprop_ratio1 = gave_ratio1.mean()\nprop_ratio2 = gave_ratio2.mean()\nprop_ratio3 = gave_ratio3.mean()\n\nprint(\"Direct Response Rate Differences:\")\nprint(f\"1:1 match: {prop_ratio1:.4%}\")\nprint(f\"2:1 match: {prop_ratio2:.4%}\")\nprint(f\"3:1 match: {prop_ratio3:.4%}\")\nprint(f\"\\n(2:1 - 1:1): {(prop_ratio2 - prop_ratio1):.4%}\")\nprint(f\"(3:1 - 2:1): {(prop_ratio3 - prop_ratio2):.4%}\")\n\nDirect Response Rate Differences:\n1:1 match: 2.0749%\n2:1 match: 2.2633%\n3:1 match: 2.2733%\n\n(2:1 - 1:1): 0.1884%\n(3:1 - 2:1): 0.0100%\n\n\nInterpretation: The direct response rate increases from 1 : 1 to 2 : 1 and from 2 : 1 to 3 : 1 are only 0.1884 percent and 0.0100 percent, respectively. These minimal changes confirm that the effect of increasing the match ratio is very small. The regression results mirror this finding, with similar small differences in the fitted coefficients. In conclusion, the size of the matched donation offer does not meaningfully affect donor response rates, reinforcing the idea that the presence of a match matters more than how generous it is.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n1. Regression on Full Sample\n\nreg_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(\"Regression of Donation Amount on Treatment (Full Sample):\")\nprint(reg_amount.summary().tables[1])\n\nRegression of Donation Amount on Treatment (Full Sample):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nInterpretation: In the full sample, the regression shows that the treatment group donated slightly more on average than the control group. The coefficient on the treatment variable is 0.1536, suggesting a small positive effect, but it is not statistically significant at the 5 percent level (p = 0.063). This means we cannot confidently conclude that offering a matching donation increased average donation amounts. The result suggests a weak effect that may be driven by the increased number of donors in the treatment group, rather than larger donation sizes.\n\n\n\n2. Regression Among Donors Only\n\ndf_donors = df[df[\"amount\"] &gt; 0]\nreg_amount_donors = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\nprint(\"Regression of Donation Amount on Treatment (Donors Only):\")\nprint(reg_amount_donors.summary().tables[1])\n\nRegression of Donation Amount on Treatment (Donors Only):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nInterpretation: Among those who donated, the regression shows that the treatment group gave slightly less on average than the control group, but the difference is not statistically significant. This suggests that while the treatment may affect the decision to donate, it does not meaningfully change how much people give once they decide to donate. The result is descriptive and should not be interpreted as causal.\n\n\n\n3. Histograms of Donation Amounts (Donors Only)\n\nimport matplotlib.pyplot as plt\n\ndonations_treatment = df_donors.loc[df_donors[\"treatment\"] == 1, \"amount\"]\ndonations_control = df_donors.loc[df_donors[\"treatment\"] == 0, \"amount\"]\n\nmean_treatment = donations_treatment.mean()\nmean_control = donations_control.mean()\n\n# Histogram for Treatment group\nplt.figure()\nplt.hist(donations_treatment, bins=30)\nplt.axvline(mean_treatment, color='red', linewidth=2, label=f\"Mean = {mean_treatment:.2f}\")\nplt.title(\"Histogram of Donation Amounts (Treatment, Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Histogram for Control group\nplt.figure()\nplt.hist(donations_control, bins=30)\nplt.axvline(mean_control, color='red', linewidth=2, label=f\"Mean = {mean_control:.2f}\")\nplt.title(\"Histogram of Donation Amounts (Control, Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The two histograms show the distribution of donation amounts among people who donated, with a red vertical line indicating the group mean.\nThe distributions are similarly right-skewed, and the treatment group shows a slightly lower mean. This visual pattern supports the regression result: among donors, the treatment did not increase donation size and may have slightly reduced it, though not significantly.\nThese plots provide helpful context: while the treatment increased the probability of donating (from earlier analysis), it did not increase how much people gave when they did choose to donate."
  },
  {
    "objectID": "projects/hw1/index.html#simulation-experiment",
    "href": "projects/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)  # For reproducibility\n\n# Simulate control and treatment draws\ncontrol_full = np.random.binomial(1, 0.018, 100000)\ntreatment_sample = np.random.binomial(1, 0.022, 10000)\n\n# Sample from control for fair comparison\ncontrol_sample = np.random.choice(control_full, 10000, replace=False)\n\n# Compute paired differences\ndifferences = treatment_sample - control_sample\n\n# Cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure()\nplt.plot(cumulative_avg, label=\"Cumulative Average\")\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Difference (0.004)\")\nplt.title(\"Cumulative Average of Differences (Law of Large Numbers)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: The plot above shows the cumulative average of 10,000 simulated differences between a treatment group and a control group, where donations follow a Bernoulli distribution with probabilities of 0.022 and 0.018, respectively. Each value in the blue line represents the average difference in donation probability as we include more simulated pairs. Early on, the average fluctuates widely due to small sample noise. As the number of observations increases, the cumulative average stabilizes and gets closer to the true difference of 0.004, shown by the red dashed line. This illustrates the Law of Large Numbers, which tells us that with enough observations, the average of the observed data approaches the expected value.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\nsample_sizes = [50, 200, 500, 1000]\nn_iterations = 1000\n\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_iterations):\n        treat_sample = np.random.binomial(1, 0.022, n)\n        control_sample = np.random.binomial(1, 0.018, n)\n        diff = treat_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    \n    # Plot histogram\n    plt.figure()\n    plt.hist(diffs, bins=30)\n    plt.title(f\"Histogram of Difference in Means (n = {n})\")\n    plt.xlabel(\"Difference in Mean Donation Probability\")\n    plt.ylabel(\"Frequency\")\n    plt.axvline(np.mean(diffs), color='red', linewidth=2,\n                label=f\"Mean = {np.mean(diffs):.4f}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The four histograms above show the distribution of 1,000 simulated differences in mean donation probability between treatment and control groups, for sample sizes of 50, 200, 500, and 1000. At smaller sample sizes like 50, the distribution is wide and less symmetric, with more extreme values and visible skew. As the sample size increases, the distribution becomes tighter and more bell-shaped, concentrating around the true mean difference. This reflects the Central Limit Theorem, which states that as sample size grows, the sampling distribution of the mean difference approaches a normal distribution. In each plot, the red vertical line represents the sample mean. At larger sample sizes like 500 and 1000, zero is clearly not at the center of the distribution, which shows that the observed difference in donation rates is unlikely to be due to chance alone when sample sizes are large."
  },
  {
    "objectID": "projects/hw3/index.html",
    "href": "projects/hw3/index.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "projects/hw3/index.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "projects/hw3/index.html#simulate-conjoint-data",
    "href": "projects/hw3/index.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport numpy as np\nimport pandas as pd\nimport itertools\n\nnp.random.seed(123)\nbrand = [\"N\", \"P\", \"H\"]  \nad = [\"Yes\", \"No\"]\nprice = np.arange(8, 33, 4)\n\nprofiles = pd.DataFrame(list(itertools.product(brand, ad, price)), columns=[\"brand\", \"ad\", \"price\"])\nm = len(profiles)\n\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\ndef sim_one(id):\n    datlist = []\n\n    for t in range(1, n_tasks + 1):\n        sampled_profiles = profiles.sample(n=n_alts).copy()\n        sampled_profiles[\"resp\"] = id\n        sampled_profiles[\"task\"] = t\n\n        sampled_profiles[\"v\"] = sampled_profiles[\"brand\"].map(b_util) + \\\n                                sampled_profiles[\"ad\"].map(a_util) + \\\n                                sampled_profiles[\"price\"].apply(p_util)\n        sampled_profiles[\"v\"] = sampled_profiles[\"v\"].round(10)\n        sampled_profiles[\"e\"] = -np.log(-np.log(np.random.rand(n_alts)))\n        sampled_profiles[\"u\"] = sampled_profiles[\"v\"] + sampled_profiles[\"e\"]\n        sampled_profiles[\"choice\"] = (sampled_profiles[\"u\"] == sampled_profiles[\"u\"].max()).astype(int)\n\n        datlist.append(sampled_profiles)\n\n    return pd.concat(datlist)\n\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]\n\nprint(conjoint_data.head())\n\n   resp  task brand  ad  price  choice\n0     1     1     P  No     32       0\n1     1     1     N  No     28       0\n2     1     1     N  No     24       1\n3     1     2     H  No     28       0\n4     1     2     H  No      8       1"
  },
  {
    "objectID": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "href": "projects/hw3/index.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\n\n# One-hot encode brand and ad\nconjoint_data = pd.read_csv('conjoint_data.csv')\ndf = conjoint_data.copy()\ndf = pd.get_dummies(df, columns=['brand','ad'], drop_first=False)\n\n# Rename columns for clarity\ndf.rename(columns={'brand_N':'beta_netflix', 'brand_P':'beta_prime', 'ad_Yes':'beta_ads'}, inplace=True)\n\ndf.sort_values(['resp','task'], inplace=True)\ndf.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nbrand_H\nbeta_netflix\nbeta_prime\nad_No\nbeta_ads\n\n\n\n\n0\n1\n1\n1\n28\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n1\n1\n1\n0\n16\nTrue\nFalse\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n0\n16\nFalse\nFalse\nTrue\nFalse\nTrue\n\n\n3\n1\n2\n0\n32\nFalse\nTrue\nFalse\nFalse\nTrue\n\n\n4\n1\n2\n1\n16\nFalse\nFalse\nTrue\nFalse\nTrue"
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "href": "projects/hw3/index.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe define the negative log‐likelihood and optimize with scipy.optimize.minimize. Standard errors come from the inverse Hessian. We then compute 95% confidence intervals.\n\nfrom scipy.optimize import minimize\n\n# Design matrix and response\nX = df[['beta_netflix','beta_prime','beta_ads','price']].to_numpy(dtype=float)\ny = df['choice'].to_numpy(dtype=int)\nn_alt = 3\nn_obs = len(y) // n_alt\n\n# Define negative log‐likelihood\ndef neg_loglik(beta):\n    # reshape X into (n_obs, n_alt, n_cov)\n    XB = X.reshape(n_obs, n_alt, -1)          \n    utility = XB @ beta                     \n    exp_u = np.exp(utility)  \n    probs = exp_u / exp_u.sum(axis=1, keepdims=True)\n    y_mat = y.reshape(n_obs, n_alt)\n    ll = np.sum(y_mat * np.log(probs))\n    return -ll\n\n# Optimize with BFGS\ninit = np.zeros(4, dtype=float)\nres = minimize(neg_loglik, init, method='BFGS')\n\n# Extract estimates, standard errors, and CIs\nbeta_hat = res.x\nhess_inv = res.hess_inv             \nse = np.sqrt(np.diag(hess_inv))\nci_lower = beta_hat - 1.96*se\nci_upper = beta_hat + 1.96*se\n\nresults_mle = pd.DataFrame({\n    'Estimate':    beta_hat,\n    'Std. Error':  se,\n    'CI 2.5%':     ci_lower,\n    'CI 97.5%':    ci_upper\n}, index=['Netflix','Prime','Ads','Price'])\n\nprint(results_mle)\n\n         Estimate  Std. Error   CI 2.5%  CI 97.5%\nNetflix  0.941195    0.113982  0.717790  1.164600\nPrime    0.501616    0.120844  0.264761  0.738471\nAds     -0.731994    0.088593 -0.905637 -0.558352\nPrice   -0.099480    0.006358 -0.111942 -0.087019"
  },
  {
    "objectID": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "href": "projects/hw3/index.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nNow we’ll implement a Metropolis-Hastings MCMC sampler to estimate the posterior distribution of the parameters.\n\nimport matplotlib.pyplot as plt\n\n# Define log‐prior\ndef log_prior(beta):\n    # beta[0:3] ~ N(0,5), beta[3] ~ N(0,1)\n    lp = -0.5 * (beta[0]**2/5 + beta[1]**2/5 + beta[2]**2/5 + beta[3]**2/1)\n    return lp\n\n# Define log‐posterior (up to constant)\ndef log_post(beta):\n    return -neg_loglik(beta) + log_prior(beta)\n\n# MCMC settings\nn_iter = 11000\nburn_in = 1000\ndraws = np.zeros((n_iter, 4))\ndraws[0] = beta_hat \nscales = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Run Metropolis–Hastings\nfor t in range(1, n_iter):\n    cand = draws[t-1] + np.random.randn(4) * scales\n    log_alpha = log_post(cand) - log_post(draws[t-1])\n    if np.log(np.random.rand()) &lt; log_alpha:\n        draws[t] = cand\n    else:\n        draws[t] = draws[t-1]\n\n# Discard burn-in\nsamples = draws[burn_in:]\n\nCreate trace plots and posterior distributions for Netflix’s parameter:\n\n# Trace plot & histogram for Netflix beta\nplt.figure(figsize=(10,4))\nplt.subplot(1,2,1)\nplt.plot(samples[:,0], linewidth=0.5)\nplt.title('Trace: β_Netflix')\nplt.xlabel('Iteration')\nplt.ylabel('Value')\n\nplt.subplot(1,2,2)\nplt.hist(samples[:,0], bins=30, edgecolor='k')\nplt.title('Posterior: β_Netflix')\nplt.xlabel('Value')\nplt.ylabel('Frequency')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nCompare the MLE and Bayesian results:\n\n# Summarize posterior for all 4 betas\ndf_post = pd.DataFrame(samples, columns=['Netflix','Prime','Ads','Price'])\nsummary_bayes = pd.DataFrame({\n    'Mean': df_post.mean(),\n    'Std.Dev': df_post.std(),\n    '2.5%': df_post.quantile(0.025),\n    '97.5%': df_post.quantile(0.975)\n})\nprint(summary_bayes)\n\n             Mean   Std.Dev      2.5%     97.5%\nNetflix  0.930445  0.110046  0.718276  1.149962\nPrime    0.490391  0.110218  0.278346  0.709853\nAds     -0.724630  0.088576 -0.894211 -0.542911\nPrice   -0.099541  0.006426 -0.112018 -0.086831"
  },
  {
    "objectID": "projects/hw3/index.html#discussion",
    "href": "projects/hw3/index.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\n\nInterpretation of Parameter Estimates\nEven if you hadn’t generated the data yourself, the estimated coefficients still tell you how each attribute drives choice. In both the MLE and Bayesian results:\nβNetflix ≈ 0.94 and βPrime ≈ 0.50: Thus respondents derive more utility from Netflix than from Amazon Prime. Formally, βNetflix &gt; βPrime means that, all else equal, the probability of choosing Netflix exceeds that of choosing Prime. Both methods yielded nearly identical results (MLE: 0.941 vs Bayesian: 0.930 for Netflix; MLE: 0.502 vs Bayesian: 0.490 for Prime), demonstrating the robustness of both estimation approaches.\nβAds ≈ −0.73 shows that having ads reduces utility relative to an ad-free experience. This substantial negative effect indicates that advertisements significantly diminish the attractiveness of streaming services.\nβPrice ≈ −0.10 is negative, which makes perfect sense: higher monthly cost lowers the attractiveness of a streaming option. Each additional dollar reduces utility by approximately 0.1 units, suggesting consumers are reasonably price-sensitive in this subscription market. The tight confidence/credible intervals around these estimates (none straddling zero) confirm that these effects are both statistically and substantively meaningful.\n\n\nToward a Hierarchical (Random-Parameter) MNL\nA multi-level or random-parameter MNL allows each respondent i to have their own taste vector βi, drawn from a population distribution:\n\\[\\beta_i \\sim N(\\mu, \\Sigma)\\]\nTo simulate such data, you would first draw each individual’s βi from N(μ, Σ), then for each choice task compute utilities and simulate choices using that person-specific βi.\nTo estimate the hierarchical model you’d:\n\nIntroduce hyperpriors on μ and Σ, specifying prior distributions for both the population means and the variance-covariance matrix.\nImplement a Bayesian sampler (e.g., Gibbs or HMC) that cycles between updating each respondent’s βi (conditional on data and μ, Σ) and updating μ, Σ (conditional on all βi). Estimation becomes computationally intensive because the likelihood requires integration over the random effects distribution for each individual.\n\nThis extension captures heterogeneity in preferences (“some people love Netflix much more than others”) and is the standard approach for real-world conjoint analysis because it enables market segmentation, individual-level predictions, and more realistic uncertainty quantification about consumer behavior."
  },
  {
    "objectID": "projects/hw4-2a/index.html",
    "href": "projects/hw4-2a/index.html",
    "title": "K Nearest Neighbors Classification",
    "section": "",
    "text": "In this section, I explore the K-Nearest Neighbors (KNN) algorithm, a simple yet effective supervised learning method used for classification tasks. KNN is a non-parametric method that makes predictions based on the majority class of the nearest data points in the feature space.\nTo illustrate the workings of KNN, I generate a synthetic dataset with two features (x1 and x2) and a binary target variable (y). The class boundary is designed to be non-linear and wiggly, defined by a sine function, making it a useful benchmark for testing the behavior of KNN under non-linear decision boundaries.\nI then implement the KNN algorithm from scratch, and compare its predictions with those of scikit-learn’s built-in KNeighborsClassifier. Finally, I evaluate model accuracy on a test dataset for values of ( k = 1 ) to ( k = 30 ), and determine the optimal number of neighbors by plotting test set performance.\nThroughout this exercise, the focus is on:\n\nUnderstanding the behavior of KNN with respect to the choice of ( k )\nGaining experience with both manual and library-based implementations\nEvaluating the model’s generalization using synthetic test data"
  },
  {
    "objectID": "projects/hw4-2a/index.html#introduction",
    "href": "projects/hw4-2a/index.html#introduction",
    "title": "K Nearest Neighbors Classification",
    "section": "",
    "text": "In this section, I explore the K-Nearest Neighbors (KNN) algorithm, a simple yet effective supervised learning method used for classification tasks. KNN is a non-parametric method that makes predictions based on the majority class of the nearest data points in the feature space.\nTo illustrate the workings of KNN, I generate a synthetic dataset with two features (x1 and x2) and a binary target variable (y). The class boundary is designed to be non-linear and wiggly, defined by a sine function, making it a useful benchmark for testing the behavior of KNN under non-linear decision boundaries.\nI then implement the KNN algorithm from scratch, and compare its predictions with those of scikit-learn’s built-in KNeighborsClassifier. Finally, I evaluate model accuracy on a test dataset for values of ( k = 1 ) to ( k = 30 ), and determine the optimal number of neighbors by plotting test set performance.\nThroughout this exercise, the focus is on:\n\nUnderstanding the behavior of KNN with respect to the choice of ( k )\nGaining experience with both manual and library-based implementations\nEvaluating the model’s generalization using synthetic test data"
  },
  {
    "objectID": "projects/hw4-2a/index.html#dataset-generation",
    "href": "projects/hw4-2a/index.html#dataset-generation",
    "title": "K Nearest Neighbors Classification",
    "section": "Dataset Generation",
    "text": "Dataset Generation\nI begin by generating a synthetic dataset using two features x1 and x2, and a binary outcome y. The boundary between the classes is defined by a sine function of x1.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\n\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nboundary = np.sin(4 * x1) + x1\ny = np.where(x2 &gt; boundary, 1, 0).astype(str)\ny = pd.Categorical(y)\n\ndata = pd.DataFrame({'x1': x1, 'x2': x2, 'y': y})\ndata.head()\n\n\n\n\n\n\n\n\nx1\nx2\ny\n\n\n\n\n0\n-0.752759\n-2.811425\n0\n\n\n1\n2.704286\n0.818462\n0\n\n\n2\n1.391964\n-1.113864\n0\n\n\n3\n0.591951\n0.051424\n0\n\n\n4\n-2.063888\n2.445399\n1"
  },
  {
    "objectID": "projects/hw4-2a/index.html#visualization-of-training-data",
    "href": "projects/hw4-2a/index.html#visualization-of-training-data",
    "title": "K Nearest Neighbors Classification",
    "section": "Visualization of Training Data",
    "text": "Visualization of Training Data\n\ncolors = {'0': 'blue', '1': 'red'}\nplt.figure(figsize=(8,6))\nplt.scatter(data['x1'], data['x2'], c=[colors[i] for i in data['y']], label='Training Data')\nx_vals = np.linspace(-3, 3, 300)\nplt.plot(x_vals, np.sin(4 * x_vals) + x_vals, color='black', linestyle='--', label='True Boundary')\nplt.xlabel('x1')\nplt.ylabel('x2')\nplt.title('Training Data with Decision Boundary')\nplt.legend()\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "projects/hw4-2a/index.html#test-dataset-generation",
    "href": "projects/hw4-2a/index.html#test-dataset-generation",
    "title": "K Nearest Neighbors Classification",
    "section": "Test Dataset Generation",
    "text": "Test Dataset Generation\n\nnp.random.seed(99)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = np.where(x2_test &gt; boundary_test, 1, 0).astype(str)\ny_test = pd.Categorical(y_test)\n\ntest_data = pd.DataFrame({'x1': x1_test, 'x2': x2_test, 'y': y_test})"
  },
  {
    "objectID": "projects/hw4-2a/index.html#manual-knn-implementation",
    "href": "projects/hw4-2a/index.html#manual-knn-implementation",
    "title": "K Nearest Neighbors Classification",
    "section": "Manual KNN Implementation",
    "text": "Manual KNN Implementation\n\nfrom collections import Counter\n\ndef euclidean(p1, p2):\n    return np.sqrt(np.sum((p1 - p2)**2))\n\ndef knn_predict(X_train, y_train, X_test, k):\n    predictions = []\n    for test_point in X_test:\n        distances = [euclidean(test_point, train_point) for train_point in X_train]\n        k_indices = np.argsort(distances)[:k]\n        k_labels = y_train[k_indices]\n        majority = Counter(k_labels).most_common(1)[0][0]\n        predictions.append(majority)\n    return np.array(predictions)"
  },
  {
    "objectID": "projects/hw4-2a/index.html#evaluation-and-comparison-with-scikit-learn",
    "href": "projects/hw4-2a/index.html#evaluation-and-comparison-with-scikit-learn",
    "title": "K Nearest Neighbors Classification",
    "section": "Evaluation and Comparison with scikit-learn",
    "text": "Evaluation and Comparison with scikit-learn\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\nX_train = data[['x1', 'x2']].values\ny_train = data['y'].astype(str).values\nX_test = test_data[['x1', 'x2']].values\ny_true = test_data['y'].astype(str).values\n\naccuracies_manual = []\naccuracies_sklearn = []\n\nfor k in range(1, 31):\n    y_pred_manual = knn_predict(X_train, y_train, X_test, k)\n    acc_manual = accuracy_score(y_true, y_pred_manual)\n    accuracies_manual.append(acc_manual)\n\n    clf = KNeighborsClassifier(n_neighbors=k)\n    clf.fit(X_train, y_train)\n    acc_sklearn = clf.score(X_test, y_true)\n    accuracies_sklearn.append(acc_sklearn)"
  },
  {
    "objectID": "projects/hw4-2a/index.html#accuracy-plot",
    "href": "projects/hw4-2a/index.html#accuracy-plot",
    "title": "K Nearest Neighbors Classification",
    "section": "Accuracy Plot",
    "text": "Accuracy Plot\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1, 31), np.array(accuracies_manual)*100, label='Manual KNN', marker='o')\nplt.plot(range(1, 31), np.array(accuracies_sklearn)*100, label='Sklearn KNN', marker='x')\nplt.xlabel('k')\nplt.ylabel('Accuracy (%)')\nplt.title('KNN Accuracy on Test Set for k=1 to 30')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\n\nOverall Accuracy Trend:\nBoth implementations show high accuracy for small ( k ), with performance gradually decreasing as ( k ) increases.\nManual vs. Sklearn:\nThe manual KNN tends to slightly outperform scikit-learn’s implementation for most values of ( k ), possibly due to tie-breaking differences.\nStability:\nThe accuracy of manual KNN remains around 90–92% for ( k ), showing stable performance before declining more consistently."
  },
  {
    "objectID": "projects/hw4-2a/index.html#conclusion",
    "href": "projects/hw4-2a/index.html#conclusion",
    "title": "K Nearest Neighbors Classification",
    "section": "Conclusion",
    "text": "Conclusion\nFrom the plot of test accuracies, the highest accuracy achieved was 92%, occurring at multiple values including k=1, 2, 4, 6, 13 and 16. While k=1 yields the best performance, it is prone to overfitting since it depends entirely on the nearest point. The manual implementation consistently performs marginally better than the scikit-learn version, likely due to subtle differences in tie-breaking or distance calculations. Overall, selecting a small odd k, such as 4 or 6, provides a balanced approach that maintains high accuracy while enhancing model robustness."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  }
]