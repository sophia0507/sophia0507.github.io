[
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "My Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/pyzmq-26.4.0.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2009-2012, Brian Granger, Min Ragan-Kelley\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/numpy/random/LICENSE.html",
    "title": "NCSA Open Source License",
    "section": "",
    "text": "This software is dual-licensed under the The University of Illinois/NCSA Open Source License (NCSA) and The 3-Clause BSD License\n\nNCSA Open Source License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nDeveloped by: Kevin Sheppard (kevin.sheppard@economics.ox.ac.uk, kevin.k.sheppard@gmail.com) http://www.kevinsheppard.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal with the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimers.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimers in the documentation and/or other materials provided with the distribution.\nNeither the names of Kevin Sheppard, nor the names of any contributors may be used to endorse or promote products derived from this Software without specific prior written permission.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE CONTRIBUTORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS WITH THE SOFTWARE.\n\n\n3-Clause BSD License\nCopyright (c) 2019 Kevin Sheppard. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n\nComponents\nMany parts of this module have been derived from original sources, often the algorithm’s designer. Component licenses are located with the component code."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/httpx-0.28.1.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright © 2019, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "projects/hw1/index.html",
    "href": "projects/hw1/index.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. I assess the balance of the experimental randomization, compare donation rates (and donation amounts), and finally use simulation to illustrate the Law of Large Numbers and the Central Limit Theorem. The following sections detail our approach, findings, and conclusions."
  },
  {
    "objectID": "projects/hw1/index.html#introduction",
    "href": "projects/hw1/index.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. I assess the balance of the experimental randomization, compare donation rates (and donation amounts), and finally use simulation to illustrate the Law of Large Numbers and the Central Limit Theorem. The following sections detail our approach, findings, and conclusions."
  },
  {
    "objectID": "projects/hw1/index.html#data",
    "href": "projects/hw1/index.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\nprint(\"Data Info:\")\nprint(df.info())\n\nprint(\"\\nFirst 5 Rows:\")\nprint(df.head())\n\nprint(\"\\nDescriptive Statistics:\")\nprint(df.describe())\n\nData Info:\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 50083 entries, 0 to 50082\nData columns (total 51 columns):\n #   Column              Non-Null Count  Dtype   \n---  ------              --------------  -----   \n 0   treatment           50083 non-null  int8    \n 1   control             50083 non-null  int8    \n 2   ratio               50083 non-null  category\n 3   ratio2              50083 non-null  int8    \n 4   ratio3              50083 non-null  int8    \n 5   size                50083 non-null  category\n 6   size25              50083 non-null  int8    \n 7   size50              50083 non-null  int8    \n 8   size100             50083 non-null  int8    \n 9   sizeno              50083 non-null  int8    \n 10  ask                 50083 non-null  category\n 11  askd1               50083 non-null  int8    \n 12  askd2               50083 non-null  int8    \n 13  askd3               50083 non-null  int8    \n 14  ask1                50083 non-null  int16   \n 15  ask2                50083 non-null  int16   \n 16  ask3                50083 non-null  int16   \n 17  amount              50083 non-null  float32 \n 18  gave                50083 non-null  int8    \n 19  amountchange        50083 non-null  float32 \n 20  hpa                 50083 non-null  float32 \n 21  ltmedmra            50083 non-null  int8    \n 22  freq                50083 non-null  int16   \n 23  years               50082 non-null  float64 \n 24  year5               50083 non-null  int8    \n 25  mrm2                50082 non-null  float64 \n 26  dormant             50083 non-null  int8    \n 27  female              48972 non-null  float64 \n 28  couple              48935 non-null  float64 \n 29  state50one          50083 non-null  int8    \n 30  nonlit              49631 non-null  float64 \n 31  cases               49631 non-null  float64 \n 32  statecnt            50083 non-null  float32 \n 33  stateresponse       50083 non-null  float32 \n 34  stateresponset      50083 non-null  float32 \n 35  stateresponsec      50080 non-null  float32 \n 36  stateresponsetminc  50080 non-null  float32 \n 37  perbush             50048 non-null  float32 \n 38  close25             50048 non-null  float64 \n 39  red0                50048 non-null  float64 \n 40  blue0               50048 non-null  float64 \n 41  redcty              49978 non-null  float64 \n 42  bluecty             49978 non-null  float64 \n 43  pwhite              48217 non-null  float32 \n 44  pblack              48047 non-null  float32 \n 45  page18_39           48217 non-null  float32 \n 46  ave_hh_sz           48221 non-null  float32 \n 47  median_hhincome     48209 non-null  float64 \n 48  powner              48214 non-null  float32 \n 49  psch_atlstba        48215 non-null  float32 \n 50  pop_propurban       48217 non-null  float32 \ndtypes: category(3), float32(16), float64(12), int16(4), int8(16)\nmemory usage: 8.9 MB\nNone\n\nFirst 5 Rows:\n   treatment  control    ratio  ratio2  ratio3      size  size25  size50  \\\n0          0        1  Control       0       0   Control       0       0   \n1          0        1  Control       0       0   Control       0       0   \n2          1        0        1       0       0  $100,000       0       0   \n3          1        0        1       0       0  Unstated       0       0   \n4          1        0        1       0       0   $50,000       0       1   \n\n   size100  sizeno  ... redcty  bluecty    pwhite    pblack  page18_39  \\\n0        0       0  ...    0.0      1.0  0.446493  0.527769   0.317591   \n1        0       0  ...    1.0      0.0       NaN       NaN        NaN   \n2        1       0  ...    0.0      1.0  0.935706  0.011948   0.276128   \n3        0       1  ...    1.0      0.0  0.888331  0.010760   0.279412   \n4        0       0  ...    0.0      1.0  0.759014  0.127421   0.442389   \n\n   ave_hh_sz  median_hhincome    powner  psch_atlstba  pop_propurban  \n0       2.10          28517.0  0.499807      0.324528            1.0  \n1        NaN              NaN       NaN           NaN            NaN  \n2       2.48          51175.0  0.721941      0.192668            1.0  \n3       2.65          79269.0  0.920431      0.412142            1.0  \n4       1.85          40908.0  0.416072      0.439965            1.0  \n\n[5 rows x 51 columns]\n\nDescriptive Statistics:\n          treatment       control        ratio2        ratio3        size25  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.666813      0.333187      0.222311      0.222211      0.166723   \nstd        0.471357      0.471357      0.415803      0.415736      0.372732   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        1.000000      0.000000      0.000000      0.000000      0.000000   \n75%        1.000000      1.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n             size50       size100        sizeno         askd1         askd2  \\\ncount  50083.000000  50083.000000  50083.000000  50083.000000  50083.000000   \nmean       0.166623      0.166723      0.166743      0.222311      0.222291   \nstd        0.372643      0.372732      0.372750      0.415803      0.415790   \nmin        0.000000      0.000000      0.000000      0.000000      0.000000   \n25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n75%        0.000000      0.000000      0.000000      0.000000      0.000000   \nmax        1.000000      1.000000      1.000000      1.000000      1.000000   \n\n       ...        redcty       bluecty        pwhite        pblack  \\\ncount  ...  49978.000000  49978.000000  48217.000000  48047.000000   \nmean   ...      0.510245      0.488715      0.819599      0.086710   \nstd    ...      0.499900      0.499878      0.168561      0.135868   \nmin    ...      0.000000      0.000000      0.009418      0.000000   \n25%    ...      0.000000      0.000000      0.755845      0.014729   \n50%    ...      1.000000      0.000000      0.872797      0.036554   \n75%    ...      1.000000      1.000000      0.938827      0.090882   \nmax    ...      1.000000      1.000000      1.000000      0.989622   \n\n          page18_39     ave_hh_sz  median_hhincome        powner  \\\ncount  48217.000000  48221.000000     48209.000000  48214.000000   \nmean       0.321694      2.429012     54815.700533      0.669418   \nstd        0.103039      0.378115     22027.316665      0.193405   \nmin        0.000000      0.000000      5000.000000      0.000000   \n25%        0.258311      2.210000     39181.000000      0.560222   \n50%        0.305534      2.440000     50673.000000      0.712296   \n75%        0.369132      2.660000     66005.000000      0.816798   \nmax        0.997544      5.270000    200001.000000      1.000000   \n\n       psch_atlstba  pop_propurban  \ncount  48215.000000   48217.000000  \nmean       0.391661       0.871968  \nstd        0.186599       0.258654  \nmin        0.000000       0.000000  \n25%        0.235647       0.884929  \n50%        0.373744       1.000000  \n75%        0.530036       1.000000  \nmax        1.000000       1.000000  \n\n[8 rows x 48 columns]\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\n\nimport pandas as pd\nfrom scipy import stats\nimport statsmodels.formula.api as smf\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\ncontrol_mrm2 = df.loc[df[\"treatment\"] == 0, \"mrm2\"]\ntreatment_mrm2 = df.loc[df[\"treatment\"] == 1, \"mrm2\"]\n\n# t-test\nt_stat, p_val = stats.ttest_ind(treatment_mrm2, control_mrm2, nan_policy=\"omit\")\nprint(\"T-test for mrm2 (months since last donation) by treatment group\")\nprint(f\"t-statistic: {t_stat:.4f}, p-value: {p_val:.4f}\")\n\n# linear regression of mrm2 on treatment\nreg_balance = smf.ols(\"mrm2 ~ treatment\", data=df).fit()\nprint(\"\\nLinear Regression Output (mrm2 ~ treatment):\")\nprint(reg_balance.summary().tables[1])\n\nT-test for mrm2 (months since last donation) by treatment group\nt-statistic: 0.1195, p-value: 0.9049\n\nLinear Regression Output (mrm2 ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9981      0.094    138.979      0.000      12.815      13.181\ntreatment      0.0137      0.115      0.119      0.905      -0.211       0.238\n==============================================================================\n\n\nInterpretation: Both the t-test and regression show no significant difference in months since last donation between treatment and control groups (p approximately 0.905). This suggests the randomization was successful, with balanced baseline characteristics. Table 1 in the original paper serves this same purpose, to demonstrate that treatment and control groups were comparable before the intervention."
  },
  {
    "objectID": "projects/hw1/index.html#experimental-results",
    "href": "projects/hw1/index.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\n1. Barplot: Proportion Donated by Group\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# donation rates\nprop_treatment = df.loc[df[\"treatment\"] == 1, \"gave\"].mean()\nprop_control = df.loc[df[\"treatment\"] == 0, \"gave\"].mean()\n\n# plot\ngroups = [\"Control\", \"Treatment\"]\nprops = [prop_control, prop_treatment]\n\nplt.figure()\nbars = plt.bar(groups, props)\nplt.title(\"Proportion of Donation by Group\")\nplt.xlabel(\"Group\")\nplt.ylabel(\"Proportion Donated\")\nplt.ylim([0, max(props)*1.3])\n\nfor bar, prop in zip(bars, props):\n    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001,\n             f\"{prop:.3%}\", ha='center', va='bottom', fontsize=10)\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n2. T-test and Bivariate Linear Regression\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\ndf = pd.read_stata(\"karlan_list_2007.dta\")\nfrom scipy import stats\nimport statsmodels.formula.api as smf\n# t-test\ncontrol_gave = df.loc[df[\"treatment\"] == 0, \"gave\"]\ntreatment_gave = df.loc[df[\"treatment\"] == 1, \"gave\"]\n\nt_stat_gave, p_val_gave = stats.ttest_ind(treatment_gave, control_gave, nan_policy=\"omit\")\nprint(\"T-test: Did treatment increase giving?\")\nprint(f\"t-statistic = {t_stat_gave:.4f}, p-value = {p_val_gave:.4f}\")\n\n# linear regression: gave ~ treatment\nreg_gave = smf.ols(\"gave ~ treatment\", data=df).fit()\nprint(\"\\nBivariate Regression Output (gave ~ treatment):\")\nprint(reg_gave.summary().tables[1])\n\nT-test: Did treatment increase giving?\nt-statistic = 3.1014, p-value = 0.0019\n\nBivariate Regression Output (gave ~ treatment):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\ntreatment      0.0042      0.001      3.101      0.002       0.002       0.007\n==============================================================================\n\n\nInterpretation: The t-test and bivariate regression both show that individuals who received a matching donation offer were more likely to donate. The estimated increase in donation likelihood is approximately 0.4 percentage points and statistically significant. This result directly matches the response rates reported in Table 2A Panel A of the original study: 1.8 percent for the control group and 2.2 percent for the treatment group. These findings suggest that even small framing differences in fundraising appeals, like offering to match donations, can meaningfully affect people’s willingness to give.\n\n\n\n3. Probit Regression\n\nimport statsmodels.formula.api as smf\n\n# probit regression\nprobit_model = smf.probit(\"gave ~ treatment\", data=df).fit(disp=False)\nprint(\"Probit Regression Output (gave ~ treatment):\")\nprint(probit_model.summary().tables[1])\n\nProbit Regression Output (gave ~ treatment):\n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     -2.1001      0.023    -90.073      0.000      -2.146      -2.054\ntreatment      0.0868      0.028      3.113      0.002       0.032       0.141\n==============================================================================\n\n\nInterpretation: The coefficient on the treatment variable in the probit model is positive and statistically significant (p-value = 0.002). While the raw coefficient (0.0868) differs from the marginal effect reported in Table 3 of the paper (0.004), this is expected because probit models in statsmodels report coefficients on the latent index rather than marginal effects. Thus, our results successfully replicate Table 3, Column 1 in terms of direction, significance, and inference.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\n1. Create ratio1 Dummy and Summary of Donation Rates\n\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\n\n# Create dummy for 1:1 ratio group\ndf[\"ratio1\"] = 0\ndf.loc[(df[\"treatment\"] == 1) & (df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0), \"ratio1\"] = 1\n\n# Show proportions\nprint(\"Proportion donating in each match ratio group:\")\nprint(\"1:1 match:\", df.loc[df[\"ratio1\"] == 1, \"gave\"].mean())\nprint(\"2:1 match:\", df.loc[df[\"ratio2\"] == 1, \"gave\"].mean())\nprint(\"3:1 match:\", df.loc[df[\"ratio3\"] == 1, \"gave\"].mean())\n\nProportion donating in each match ratio group:\n1:1 match: 0.020749124225276205\n2:1 match: 0.0226333752469912\n3:1 match: 0.022733399227244138\n\n\n\n\n\n2. T-Tests: Compare Donation Rates Between Match Ratios\n\nfrom scipy import stats\nimport pandas as pd\ndf = pd.read_stata(\"karlan_list_2007.dta\")\ndf[\"ratio1\"] = 0\ndf.loc[(df[\"treatment\"] == 1) & (df[\"ratio2\"] == 0) & (df[\"ratio3\"] == 0), \"ratio1\"] = 1\n\n# Extract gave data\ngave_ratio1 = df.loc[df[\"ratio1\"] == 1, \"gave\"]\ngave_ratio2 = df.loc[df[\"ratio2\"] == 1, \"gave\"]\ngave_ratio3 = df.loc[df[\"ratio3\"] == 1, \"gave\"]\n\n# 1:1 vs 2:1\nt_stat_12, p_val_12 = stats.ttest_ind(gave_ratio1, gave_ratio2, nan_policy=\"omit\")\nprint(\"t-test (1:1 vs 2:1):\")\nprint(f\"t-statistic = {t_stat_12:.4f}, p-value = {p_val_12:.4f}\")\n\n# 2:1 vs 3:1\nt_stat_23, p_val_23 = stats.ttest_ind(gave_ratio2, gave_ratio3, nan_policy=\"omit\")\nprint(\"\\nt-test (2:1 vs 3:1):\")\nprint(f\"t-statistic = {t_stat_23:.4f}, p-value = {p_val_23:.4f}\")\n\nt-test (1:1 vs 2:1):\nt-statistic = -0.9650, p-value = 0.3345\n\nt-test (2:1 vs 3:1):\nt-statistic = -0.0501, p-value = 0.9600\n\n\nInterpretation: The t-tests comparing donation rates between match ratios show no statistically significant differences. The comparison between the 1 : 1 and 2 : 1 match groups has a p-value of 0.3345, and between the 2 : 1 and 3 : 1 groups, the p-value is 0.9600. These high p-values suggest that increasing the match ratio does not significantly increase the likelihood of donating. This supports the authors’ note that figures suggest the size of the match ratio has little impact on donor behavior.\n\n\n\n3. Regression: Donation ~ Match Ratio Group (OLS)\n\nimport statsmodels.formula.api as smf\n\nreg_match = smf.ols(\"gave ~ ratio1 + ratio2 + ratio3\", data=df).fit()\nprint(\"Regression Output (gave ~ ratio1 + ratio2 + ratio3):\")\nprint(reg_match.summary().tables[1])\n\nRegression Output (gave ~ ratio1 + ratio2 + ratio3):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.0179      0.001     16.225      0.000       0.016       0.020\nratio1         0.0029      0.002      1.661      0.097      -0.001       0.006\nratio2         0.0048      0.002      2.744      0.006       0.001       0.008\nratio3         0.0049      0.002      2.802      0.005       0.001       0.008\n==============================================================================\n\n\nInterpretation: The regression results show that the 2 : 1 and 3 : 1 match groups are associated with small but statistically significant increases in donation probability compared to the control group, with p-values below 0.01. The 1 : 1 group is not statistically significant. However, all coefficients are very small in magnitude, with increases less than 0.005. This suggests that although some match sizes show statistically significant differences, the practical effect is minimal. The treatment likely matters more than the exact size of the match.\n\n\n\n4. Direct Comparison of Response Rates\n\nprop_ratio1 = gave_ratio1.mean()\nprop_ratio2 = gave_ratio2.mean()\nprop_ratio3 = gave_ratio3.mean()\n\nprint(\"Direct Response Rate Differences:\")\nprint(f\"1:1 match: {prop_ratio1:.4%}\")\nprint(f\"2:1 match: {prop_ratio2:.4%}\")\nprint(f\"3:1 match: {prop_ratio3:.4%}\")\nprint(f\"\\n(2:1 - 1:1): {(prop_ratio2 - prop_ratio1):.4%}\")\nprint(f\"(3:1 - 2:1): {(prop_ratio3 - prop_ratio2):.4%}\")\n\nDirect Response Rate Differences:\n1:1 match: 2.0749%\n2:1 match: 2.2633%\n3:1 match: 2.2733%\n\n(2:1 - 1:1): 0.1884%\n(3:1 - 2:1): 0.0100%\n\n\nInterpretation: The direct response rate increases from 1 : 1 to 2 : 1 and from 2 : 1 to 3 : 1 are only 0.1884 percent and 0.0100 percent, respectively. These minimal changes confirm that the effect of increasing the match ratio is very small. The regression results mirror this finding, with similar small differences in the fitted coefficients. In conclusion, the size of the matched donation offer does not meaningfully affect donor response rates, reinforcing the idea that the presence of a match matters more than how generous it is.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\n\n1. Regression on Full Sample\n\nreg_amount = smf.ols(\"amount ~ treatment\", data=df).fit()\nprint(\"Regression of Donation Amount on Treatment (Full Sample):\")\nprint(reg_amount.summary().tables[1])\n\nRegression of Donation Amount on Treatment (Full Sample):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept      0.8133      0.067     12.063      0.000       0.681       0.945\ntreatment      0.1536      0.083      1.861      0.063      -0.008       0.315\n==============================================================================\n\n\nInterpretation: In the full sample, the regression shows that the treatment group donated slightly more on average than the control group. The coefficient on the treatment variable is 0.1536, suggesting a small positive effect, but it is not statistically significant at the 5 percent level (p = 0.063). This means we cannot confidently conclude that offering a matching donation increased average donation amounts. The result suggests a weak effect that may be driven by the increased number of donors in the treatment group, rather than larger donation sizes.\n\n\n\n2. Regression Among Donors Only\n\ndf_donors = df[df[\"amount\"] &gt; 0]\nreg_amount_donors = smf.ols(\"amount ~ treatment\", data=df_donors).fit()\nprint(\"Regression of Donation Amount on Treatment (Donors Only):\")\nprint(reg_amount_donors.summary().tables[1])\n\nRegression of Donation Amount on Treatment (Donors Only):\n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     45.5403      2.423     18.792      0.000      40.785      50.296\ntreatment     -1.6684      2.872     -0.581      0.561      -7.305       3.968\n==============================================================================\n\n\nInterpretation: Among those who donated, the regression shows that the treatment group gave slightly less on average than the control group, but the difference is not statistically significant. This suggests that while the treatment may affect the decision to donate, it does not meaningfully change how much people give once they decide to donate. The result is descriptive and should not be interpreted as causal.\n\n\n\n3. Histograms of Donation Amounts (Donors Only)\n\nimport matplotlib.pyplot as plt\n\ndonations_treatment = df_donors.loc[df_donors[\"treatment\"] == 1, \"amount\"]\ndonations_control = df_donors.loc[df_donors[\"treatment\"] == 0, \"amount\"]\n\nmean_treatment = donations_treatment.mean()\nmean_control = donations_control.mean()\n\n# Histogram for Treatment group\nplt.figure()\nplt.hist(donations_treatment, bins=30)\nplt.axvline(mean_treatment, color='red', linewidth=2, label=f\"Mean = {mean_treatment:.2f}\")\nplt.title(\"Histogram of Donation Amounts (Treatment, Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n# Histogram for Control group\nplt.figure()\nplt.hist(donations_control, bins=30)\nplt.axvline(mean_control, color='red', linewidth=2, label=f\"Mean = {mean_control:.2f}\")\nplt.title(\"Histogram of Donation Amounts (Control, Donors Only)\")\nplt.xlabel(\"Donation Amount\")\nplt.ylabel(\"Frequency\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The two histograms show the distribution of donation amounts among people who donated, with a red vertical line indicating the group mean.\nThe distributions are similarly right-skewed, and the treatment group shows a slightly lower mean. This visual pattern supports the regression result: among donors, the treatment did not increase donation size and may have slightly reduced it, though not significantly.\nThese plots provide helpful context: while the treatment increased the probability of donating (from earlier analysis), it did not increase how much people gave when they did choose to donate."
  },
  {
    "objectID": "projects/hw1/index.html#simulation-experiment",
    "href": "projects/hw1/index.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)  # For reproducibility\n\n# Simulate control and treatment draws\ncontrol_full = np.random.binomial(1, 0.018, 100000)\ntreatment_sample = np.random.binomial(1, 0.022, 10000)\n\n# Sample from control for fair comparison\ncontrol_sample = np.random.choice(control_full, 10000, replace=False)\n\n# Compute paired differences\ndifferences = treatment_sample - control_sample\n\n# Cumulative average of the differences\ncumulative_avg = np.cumsum(differences) / np.arange(1, len(differences) + 1)\n\n# Plot\nplt.figure()\nplt.plot(cumulative_avg, label=\"Cumulative Average\")\nplt.axhline(0.004, color='red', linestyle='--', label=\"True Difference (0.004)\")\nplt.title(\"Cumulative Average of Differences (Law of Large Numbers)\")\nplt.xlabel(\"Number of Observations\")\nplt.ylabel(\"Cumulative Average\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: The plot above shows the cumulative average of 10,000 simulated differences between a treatment group and a control group, where donations follow a Bernoulli distribution with probabilities of 0.022 and 0.018, respectively. Each value in the blue line represents the average difference in donation probability as we include more simulated pairs. Early on, the average fluctuates widely due to small sample noise. As the number of observations increases, the cumulative average stabilizes and gets closer to the true difference of 0.004, shown by the red dashed line. This illustrates the Law of Large Numbers, which tells us that with enough observations, the average of the observed data approaches the expected value.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(0)\n\nsample_sizes = [50, 200, 500, 1000]\nn_iterations = 1000\n\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_iterations):\n        treat_sample = np.random.binomial(1, 0.022, n)\n        control_sample = np.random.binomial(1, 0.018, n)\n        diff = treat_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    \n    # Plot histogram\n    plt.figure()\n    plt.hist(diffs, bins=30)\n    plt.title(f\"Histogram of Difference in Means (n = {n})\")\n    plt.xlabel(\"Difference in Mean Donation Probability\")\n    plt.ylabel(\"Frequency\")\n    plt.axvline(np.mean(diffs), color='red', linewidth=2,\n                label=f\"Mean = {np.mean(diffs):.4f}\")\n    plt.legend()\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInterpretation: The four histograms above show the distribution of 1,000 simulated differences in mean donation probability between treatment and control groups, for sample sizes of 50, 200, 500, and 1000. At smaller sample sizes like 50, the distribution is wide and less symmetric, with more extreme values and visible skew. As the sample size increases, the distribution becomes tighter and more bell-shaped, concentrating around the true mean difference. This reflects the Central Limit Theorem, which states that as sample size grows, the sampling distribution of the mean difference approaches a normal distribution. In each plot, the red vertical line represents the sample mean. At larger sample sizes like 500 and 1000, zero is clearly not at the center of the distribution, which shows that the observed difference in donation rates is unlikely to be due to chance alone when sample sizes are large."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ting-Yu(Sophia) Wang",
    "section": "",
    "text": "Welcome to my website!"
  },
  {
    "objectID": "projects/project1/index.html",
    "href": "projects/project1/index.html",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Let’s investigate the relationship between fuel efficiency (mpg) and engine displacement (disp) from the mtcars dataset. Those variables have a correlation of r cor(mtcars$mpg, mtcars$disp) |&gt; format(digits=2).\n\n\nHere is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header",
    "href": "projects/project1/index.html#sub-header",
    "title": "Analysis of Cars",
    "section": "",
    "text": "Here is a plot:"
  },
  {
    "objectID": "projects/project1/index.html#sub-header-1",
    "href": "projects/project1/index.html#sub-header-1",
    "title": "Analysis of Cars",
    "section": "Sub-Header",
    "text": "Sub-Header\nHere is a plot:\nimport pandas as pd import seaborn as sns import matplotlib.pyplot as plt"
  },
  {
    "objectID": "quarto-env/Lib/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/httpcore-1.0.8.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright © 2020, Encode OSS Ltd. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/idna-3.10.dist-info/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "BSD 3-Clause License\nCopyright (c) 2013-2024, Kim Davies and contributors. All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/soupsieve-2.6.dist-info/licenses/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "MIT License\nCopyright (c) 2018 - 2024 Isaac Muse isaacmuse@gmail.com\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html",
    "href": "projects/hw2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n# Histogram\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=30)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Mean and std\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"std\", \"count\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\niscustomer\n\n\n\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n\n\n\nInterpretation: The histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count. The summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# Region distribution\npd.crosstab(df[\"region\"], df[\"iscustomer\"])\n\n# Age distribution\nsns.kdeplot(data=df, x=\"age\", hue=\"iscustomer\", fill=True, common_norm=False, alpha=0.5)\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Age\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "href": "projects/hw2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n# Histogram\nsns.histplot(data=df, x=\"patents\", hue=\"iscustomer\", multiple=\"dodge\", bins=30)\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.show()\n\n# Mean and std\ndf.groupby(\"iscustomer\")[\"patents\"].agg([\"mean\", \"std\", \"count\"])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmean\nstd\ncount\n\n\niscustomer\n\n\n\n\n\n\n\n0\n3.473013\n2.225060\n1019\n\n\n1\n4.133056\n2.546846\n481\n\n\n\n\n\n\n\nInterpretation: The histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count. The summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# Region distribution\npd.crosstab(df[\"region\"], df[\"iscustomer\"])\n\n# Age distribution\nsns.kdeplot(data=df, x=\"age\", hue=\"iscustomer\", fill=True, common_norm=False, alpha=0.5)\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Age\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "href": "projects/hw2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "quarto-env/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "href": "quarto-env/Lib/site-packages/seaborn-0.13.2.dist-info/LICENSE.html",
    "title": "Sophia's Website",
    "section": "",
    "text": "Copyright (c) 2012-2023, Michael L. Waskom All rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\nNeither the name of the project nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS “AS IS” AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
  },
  {
    "objectID": "projects/hw2/index.html",
    "href": "projects/hw2/index.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import factorial\nimport scipy.stats\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\nimport scipy.optimize as sp\nimport scipy.stats as stats\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import minimize_scalar,minimize, approx_fprime\n\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Map numeric to label\ndf[\"customer_label\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Create histogram\nax = sns.histplot(data=df, x=\"patents\", hue=\"customer_label\", multiple=\"dodge\", \n                  bins=30, palette={\"Non-Customer\": \"orange\", \"Customer\": \"skyblue\"})\n\n# Add title and axis labels\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by customer status\ndf.groupby(\"customer_label\")[\"patents\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_label\nmean\nstd\ncount\n\n\n\n\n0\nCustomer\n4.133056\n2.546846\n481\n\n\n1\nNon-Customer\n3.473013\n2.225060\n1019\n\n\n\n\n\n\n\nThe histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count.\nThe summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution\nregion_ct = pd.crosstab(df[\"region\"], df[\"customer_label\"], normalize=\"columns\") * 100\nregion_ct.columns= [\"Customer (%)\", \"Non-Customer (%)\"]\nregion_ct.index.name = \"Region\"\nregion_ct = region_ct.reset_index()\nprint(region_ct.to_string(index=False))\n\n   Region  Customer (%)  Non-Customer (%)\n  Midwest      7.692308         18.351325\nNortheast     68.191268         26.790972\nNorthwest      6.029106         15.505397\n    South      7.276507         15.309127\nSouthwest     10.810811         24.043180\n\n\n\n# Age distribution\nage_summary = df.groupby(\"customer_label\")[\"age\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(2)\nage_summary = age_summary.reset_index()\nage_summary.columns = [\"Customer Status\", \"Mean Age\", \"Std Dev\", \"Min Age\", \"Max Age\"]\nage_summary\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\nStd Dev\nMin Age\nMax Age\n\n\n\n\n0\nCustomer\n26.9\n7.81\n10.0\n49.0\n\n\n1\nNon-Customer\n26.1\n6.95\n9.0\n47.5\n\n\n\n\n\n\n\n\nages = np.linspace(df[\"age\"].min(), df[\"age\"].max(), 300)\nkde_non = gaussian_kde(df.loc[df[\"customer_label\"]==\"Non-Customer\",\"age\"])(ages)\nkde_cust = gaussian_kde(df.loc[df[\"customer_label\"]==\"Customer\",\"age\"])(ages)\n\nplt.plot(ages, kde_non, label=\"Non-Customer\", color=\"tab:orange\")\nplt.plot(ages, kde_cust, label=\"Customer\",    color=\"tab:blue\")\nplt.fill_between(ages, kde_non, alpha=0.3, color=\"tab:orange\")\nplt.fill_between(ages, kde_cust, alpha=0.3, color=\"tab:blue\")\n\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nA review of the regional distribution shows that Blueprinty customers are not evenly distributed across the United States. In particular, the Northeast region accounts for the majority of customers (68.2%), while only 26.7% of non-customers are located there. In contrast, regions like the Midwest, South, and Southwest are relatively underrepresented among customers compared to non-customers. This suggests strong geographic clustering in Blueprinty’s user base, potentially reflecting targeted marketing or regional technology adoption patterns.\nIn terms of firm age, the average age of customers is slightly younger at 26.1 years (std = 6.95), compared to 26.9 years (std = 7.81) for non-customers. Although the means are close, the density plot further reveals that non-customers have a higher peak in the 24–26 age range and a more pronounced tail toward older firms (ages 35+), while customers are more concentrated around the mid-20s.\nThese observed differences in both region and age imply that Blueprinty customers may differ systematically from non-customers. Consequently, it’s important to include these covariates in the regression model to avoid attributing differences in patent counts solely to software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe now define the log-likelihood function for the Poisson model::\n\ndef poisson_loglikelihood(lambda_val, Y):\n    return -lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1)\n\ndef poisson_loglikelihood_sum(lambda_val, Y):\n    return np.sum(-lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1))\n\nNow let’s plot the log-likelihood for a range of lambda values:\n\n# Get the observed patents data\nobserved_patents = df['patents'].values\n\n# Choose a reasonable range of lambda values based on data\nlambda_range = np.linspace(0.1, 20, 100)\nll_values = np.zeros(len(lambda_range))\n\n# Calculate log-likelihood for each lambda\nfor i, lam in enumerate(lambda_range):\n    ll_values[i] = np.sum([poisson_loglikelihood(lam, y) for y in observed_patents])\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, ll_values)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Model for Different Lambda Values')\nplt.axvline(x=observed_patents.mean(), color='red', linestyle='--', \n            label=f'Sample Mean (λ_MLE = {observed_patents.mean():.2f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) for lambda in a Poisson distribution is the sample mean. Mathematically, we can show this by taking the derivative of the log-likelihood function:\n\\(\\frac{d}{d\\lambda} \\ln f(Y|\\lambda) = -1 + \\frac{Y}{\\lambda} = 0\\)\nSolving for lambda: \\(\\lambda_{MLE} = Y\\)\nFor a sample of observations, the MLE is the sample mean: \\(\\lambda_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\\)\nThe following optimization step confirms the MLE matches the sample mean:\n\n# Define the negative log-likelihood (for minimization)\ndef neg_poisson_loglikelihood_sum(lambda_val, Y):\n    return -poisson_loglikelihood_sum(lambda_val, Y)\n\n# Find the MLE using optimization\ninitial_guess = 1.0  # Starting point for optimization\nresult = minimize(neg_poisson_loglikelihood_sum, \n                  x0=[1.0], \n                  args=(observed_patents,), \n                  method='L-BFGS-B',\n                  bounds=[(1e-6, None)])\n\nprint(f\"MLE estimate for lambda: {result.x[0]:.4f}\")\nprint(f\"Sample mean: {observed_patents.mean():.4f}\")\nprint(f\"Are they equal? {np.isclose(result.x[0], observed_patents.mean())}\")\n\nMLE estimate for lambda: 3.6847\nSample mean: 3.6847\nAre they equal? True\n\n\nAs shown in the plot and confirmed by optimization, the maximum of the log-likelihood function occurs at λ = 3.6847, which is exactly the sample mean. This aligns with the known analytical result for the MLE of λ in a Poisson model. The red dashed line highlights the MLE and validates our implementation of the likelihood-based estimation approach.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by defining the log-likelihood function for the Poisson regression model:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.array(beta, dtype=np.float64)\n    Y = np.array(Y, dtype=np.float64)\n    X = np.array(X, dtype=np.float64)\n    linear_pred = np.dot(X, beta)\n    lambda_i = np.exp(np.clip(linear_pred, -30, 30))\n    ll_components = -lambda_i + Y * np.log(lambda_i)\n    return -np.sum(ll_components)\n\n\n\n\n\n# Add squared age and region dummies\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Combine into design matrix\nX_data = pd.concat([\n    pd.DataFrame({'intercept': 1}, index=df.index),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX = X_data.values.astype(np.float64)\ny = df['patents'].values.astype(np.float64)\n\n\n\n\n\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\ninitial_beta = glm_results.params\n\n\n\n\n\n\nresult_poisson_reg = minimize(\n    poisson_regression_loglikelihood,\n    initial_beta,\n    args=(y, X),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_estimates = result_poisson_reg.x\n\n         Current function value: -1790.814845\n         Iterations: 0\n         Function evaluations: 264\n         Gradient evaluations: 28\n\n\ne:\\2025 Spring - ucsd\\Sophia's Website\\quarto-env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\n\n\n\n\n\ndef hessian(func, x, epsilon=1e-5):\n    n = len(x)\n    hess = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            x1, x2, x3, x4 = x.copy(), x.copy(), x.copy(), x.copy()\n            if i == j:\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                hess[i, i] = (func(x1) + func(x2) - 2 * func(x)) / (epsilon ** 2)\n            else:\n                x1[i] += epsilon; x1[j] += epsilon\n                x2[i] += epsilon; x2[j] -= epsilon\n                x3[i] -= epsilon; x3[j] += epsilon\n                x4[i] -= epsilon; x4[j] -= epsilon\n                hess[i, j] = (func(x1) + func(x4) - func(x2) - func(x3)) / (4 * epsilon ** 2)\n                hess[j, i] = hess[i, j]\n    return hess\n\nH = hessian(lambda b: poisson_regression_loglikelihood(b, y, X), beta_estimates)\ncov_matrix = np.linalg.inv(H)\nstd_errors = np.sqrt(np.diag(cov_matrix))\n\nz_values = beta_estimates / std_errors\np_values = 2 * (1 - np.abs(scipy.stats.norm.cdf(z_values)))\n\ncustom_results = pd.DataFrame({\n    'Coefficient': beta_estimates,\n    'Std. Error': std_errors,\n    'z-value': z_values,\n    'p-value': p_values\n}, index=X_data.columns)\n\ncustom_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1837\n-2.7698\n1.9944\n\n\nage\n0.1486\n0.0139\n10.6800\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.4738\n2.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7190\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n1.2562\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\n\n\n\n\nsm_results = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z-value': glm_results.tvalues,\n    'p-value': glm_results.pvalues\n}, index=X_data.columns)\n\nsm_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe estimates and standard errors obtained using scipy.optimize match those from statsmodels, confirming the correctness of our custom likelihood implementation.\n\n\n\nThe model successfully estimated the impact of several firm-level characteristics on patent success. we found:\n\nFirm Age: A positive and significant effect (age coefficient = 0.1486, p &lt; 0.001), with a small negative effect from age squared. This suggests diminishing returns to age: patent counts increase with age but at a decreasing rate.\nRegion: None of the region coefficients were statistically significant at the 5% level, indicating little evidence of regional differences in patent activity once other variables are controlled for.\nBlueprinty Software (iscustomer): The coefficient for iscustomer is 0.2076 with a p-value of &lt; 0.001, meaning it is both positive and highly statistically significant.\n\n\n\n\n\nTo interpret the effect of the binary variable iscustomer, we simulate two scenarios: - (X_0): all firms are non-customers (iscustomer = 0) - (X_1): all firms are customers (iscustomer = 1)\nwe compare the average predicted number of patents under each scenario:\n\nX_0 = X.copy()\nX_1 = X.copy()\niscustomer_idx = list(X_data.columns).index('iscustomer')\nX_0[:, iscustomer_idx] = 0\nX_1[:, iscustomer_idx] = 1\n\nlambda_0 = np.exp(np.dot(X_0, beta_estimates))\nlambda_1 = np.exp(np.dot(X_1, beta_estimates))\n\ndifferences = lambda_1 - lambda_0\navg_effect = np.mean(differences)\nperc_increase = 100 * avg_effect / np.mean(lambda_0)\n\nprint(\"Average predicted increase in patents:\", round(avg_effect, 3), \"per firm\")\nprint(\"Percent increase:\", round(perc_increase, 2), \"%\")\n\nAverage predicted increase in patents: 0.793 per firm\nPercent increase: 23.07 %\n\n\nThis means that, holding all else constant, firms using Blueprinty’s software are predicted to earn about 0.79 more patents over 5 years, which represents a 23% improvement compared to non-customers.\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(differences, bins=30, alpha=0.7)\nplt.axvline(avg_effect, color='red', linestyle='--', \n            label=f'Avg Effect: {avg_effect:.2f} patents')\nplt.xlabel('Increase in Predicted Patents')\nplt.ylabel('Number of Firms')\nplt.title('Effect of Using Blueprinty Software')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that nearly all firms are expected to benefit from adopting Blueprinty’s software, reinforcing the interpretation that the product positively contributes to patenting success.\nThis result supports the company’s marketing claim, though causal inference would require further analysis beyond this observational model."
  },
  {
    "objectID": "projects/hw2/index.html#blueprinty-case-study",
    "href": "projects/hw2/index.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\n\n# Import necessary libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom math import factorial\nimport scipy.stats\nfrom scipy.special import gammaln\nimport statsmodels.api as sm\nimport scipy.optimize as sp\nimport scipy.stats as stats\nfrom scipy.stats import gaussian_kde\nfrom scipy.optimize import minimize_scalar,minimize, approx_fprime\n\n\ndf = pd.read_csv(\"blueprinty.csv\")  \ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\n\n# Map numeric to label\ndf[\"customer_label\"] = df[\"iscustomer\"].map({0: \"Non-Customer\", 1: \"Customer\"})\n\n# Create histogram\nax = sns.histplot(data=df, x=\"patents\", hue=\"customer_label\", multiple=\"dodge\", \n                  bins=30, palette={\"Non-Customer\": \"orange\", \"Customer\": \"skyblue\"})\n\n# Add title and axis labels\nplt.title(\"Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.tight_layout()\nplt.show()\n\n# Summary statistics by customer status\ndf.groupby(\"customer_label\")[\"patents\"].agg([\"mean\", \"std\", \"count\"]).reset_index()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncustomer_label\nmean\nstd\ncount\n\n\n\n\n0\nCustomer\n4.133056\n2.546846\n481\n\n\n1\nNon-Customer\n3.473013\n2.225060\n1019\n\n\n\n\n\n\n\nThe histogram above illustrates the distribution of the number of patents for firms that use Blueprinty’s software (iscustomer = 1) and those that do not (iscustomer = 0). While both distributions are right-skewed and concentrated around 2 to 5 patents, Blueprinty customers generally appear to have a higher patent count.\nThe summary statistics confirm this visual pattern: the mean number of patents for non-customers is 3.47, whereas for customers it is 4.13. Additionally, customers exhibit a slightly higher standard deviation (2.55 vs. 2.23), indicating a bit more variability in patent outcomes among users of the software. These results suggest that firms using Blueprinty’s software tend to receive more patents on average than non-customers.\n\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n# Region distribution\nregion_ct = pd.crosstab(df[\"region\"], df[\"customer_label\"], normalize=\"columns\") * 100\nregion_ct.columns= [\"Customer (%)\", \"Non-Customer (%)\"]\nregion_ct.index.name = \"Region\"\nregion_ct = region_ct.reset_index()\nprint(region_ct.to_string(index=False))\n\n   Region  Customer (%)  Non-Customer (%)\n  Midwest      7.692308         18.351325\nNortheast     68.191268         26.790972\nNorthwest      6.029106         15.505397\n    South      7.276507         15.309127\nSouthwest     10.810811         24.043180\n\n\n\n# Age distribution\nage_summary = df.groupby(\"customer_label\")[\"age\"].agg([\"mean\", \"std\", \"min\", \"max\"]).round(2)\nage_summary = age_summary.reset_index()\nage_summary.columns = [\"Customer Status\", \"Mean Age\", \"Std Dev\", \"Min Age\", \"Max Age\"]\nage_summary\n\n\n\n\n\n\n\n\nCustomer Status\nMean Age\nStd Dev\nMin Age\nMax Age\n\n\n\n\n0\nCustomer\n26.9\n7.81\n10.0\n49.0\n\n\n1\nNon-Customer\n26.1\n6.95\n9.0\n47.5\n\n\n\n\n\n\n\n\nages = np.linspace(df[\"age\"].min(), df[\"age\"].max(), 300)\nkde_non = gaussian_kde(df.loc[df[\"customer_label\"]==\"Non-Customer\",\"age\"])(ages)\nkde_cust = gaussian_kde(df.loc[df[\"customer_label\"]==\"Customer\",\"age\"])(ages)\n\nplt.plot(ages, kde_non, label=\"Non-Customer\", color=\"tab:orange\")\nplt.plot(ages, kde_cust, label=\"Customer\",    color=\"tab:blue\")\nplt.fill_between(ages, kde_non, alpha=0.3, color=\"tab:orange\")\nplt.fill_between(ages, kde_cust, alpha=0.3, color=\"tab:blue\")\n\nplt.title(\"Density of Firm Age by Customer Status\")\nplt.xlabel(\"Firm Age\")\nplt.ylabel(\"Density\")\nplt.legend(title=\"Customer Status\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nA review of the regional distribution shows that Blueprinty customers are not evenly distributed across the United States. In particular, the Northeast region accounts for the majority of customers (68.2%), while only 26.7% of non-customers are located there. In contrast, regions like the Midwest, South, and Southwest are relatively underrepresented among customers compared to non-customers. This suggests strong geographic clustering in Blueprinty’s user base, potentially reflecting targeted marketing or regional technology adoption patterns.\nIn terms of firm age, the average age of customers is slightly younger at 26.1 years (std = 6.95), compared to 26.9 years (std = 7.81) for non-customers. Although the means are close, the density plot further reveals that non-customers have a higher peak in the 24–26 age range and a more pronounced tail toward older firms (ages 35+), while customers are more concentrated around the mid-20s.\nThese observed differences in both region and age imply that Blueprinty customers may differ systematically from non-customers. Consequently, it’s important to include these covariates in the regression model to avoid attributing differences in patent counts solely to software usage.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWe now define the log-likelihood function for the Poisson model::\n\ndef poisson_loglikelihood(lambda_val, Y):\n    return -lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1)\n\ndef poisson_loglikelihood_sum(lambda_val, Y):\n    return np.sum(-lambda_val + Y * np.log(lambda_val) - gammaln(Y + 1))\n\nNow let’s plot the log-likelihood for a range of lambda values:\n\n# Get the observed patents data\nobserved_patents = df['patents'].values\n\n# Choose a reasonable range of lambda values based on data\nlambda_range = np.linspace(0.1, 20, 100)\nll_values = np.zeros(len(lambda_range))\n\n# Calculate log-likelihood for each lambda\nfor i, lam in enumerate(lambda_range):\n    ll_values[i] = np.sum([poisson_loglikelihood(lam, y) for y in observed_patents])\n\n# Plot\nplt.figure(figsize=(10, 6))\nplt.plot(lambda_range, ll_values)\nplt.xlabel('Lambda')\nplt.ylabel('Log-Likelihood')\nplt.title('Log-Likelihood of Poisson Model for Different Lambda Values')\nplt.axvline(x=observed_patents.mean(), color='red', linestyle='--', \n            label=f'Sample Mean (λ_MLE = {observed_patents.mean():.2f})')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.show()\n\n\n\n\n\n\n\n\nThe maximum likelihood estimate (MLE) for lambda in a Poisson distribution is the sample mean. Mathematically, we can show this by taking the derivative of the log-likelihood function:\n\\(\\frac{d}{d\\lambda} \\ln f(Y|\\lambda) = -1 + \\frac{Y}{\\lambda} = 0\\)\nSolving for lambda: \\(\\lambda_{MLE} = Y\\)\nFor a sample of observations, the MLE is the sample mean: \\(\\lambda_{MLE} = \\frac{1}{n}\\sum_{i=1}^{n} Y_i = \\bar{Y}\\)\nThe following optimization step confirms the MLE matches the sample mean:\n\n# Define the negative log-likelihood (for minimization)\ndef neg_poisson_loglikelihood_sum(lambda_val, Y):\n    return -poisson_loglikelihood_sum(lambda_val, Y)\n\n# Find the MLE using optimization\ninitial_guess = 1.0  # Starting point for optimization\nresult = minimize(neg_poisson_loglikelihood_sum, \n                  x0=[1.0], \n                  args=(observed_patents,), \n                  method='L-BFGS-B',\n                  bounds=[(1e-6, None)])\n\nprint(f\"MLE estimate for lambda: {result.x[0]:.4f}\")\nprint(f\"Sample mean: {observed_patents.mean():.4f}\")\nprint(f\"Are they equal? {np.isclose(result.x[0], observed_patents.mean())}\")\n\nMLE estimate for lambda: 3.6847\nSample mean: 3.6847\nAre they equal? True\n\n\nAs shown in the plot and confirmed by optimization, the maximum of the log-likelihood function occurs at λ = 3.6847, which is exactly the sample mean. This aligns with the known analytical result for the MLE of λ in a Poisson model. The red dashed line highlights the MLE and validates our implementation of the likelihood-based estimation approach.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\nWe begin by defining the log-likelihood function for the Poisson regression model:\n\ndef poisson_regression_loglikelihood(beta, Y, X):\n    beta = np.array(beta, dtype=np.float64)\n    Y = np.array(Y, dtype=np.float64)\n    X = np.array(X, dtype=np.float64)\n    linear_pred = np.dot(X, beta)\n    lambda_i = np.exp(np.clip(linear_pred, -30, 30))\n    ll_components = -lambda_i + Y * np.log(lambda_i)\n    return -np.sum(ll_components)\n\n\n\n\n\n# Add squared age and region dummies\ndf[\"age_squared\"] = df[\"age\"] ** 2\nregion_dummies = pd.get_dummies(df[\"region\"], prefix=\"region\", drop_first=True)\n\n# Combine into design matrix\nX_data = pd.concat([\n    pd.DataFrame({'intercept': 1}, index=df.index),\n    df[['age', 'age_squared', 'iscustomer']],\n    region_dummies\n], axis=1)\n\nX = X_data.values.astype(np.float64)\ny = df['patents'].values.astype(np.float64)\n\n\n\n\n\n\nglm_model = sm.GLM(y, X, family=sm.families.Poisson())\nglm_results = glm_model.fit()\ninitial_beta = glm_results.params\n\n\n\n\n\n\nresult_poisson_reg = minimize(\n    poisson_regression_loglikelihood,\n    initial_beta,\n    args=(y, X),\n    method='BFGS',\n    options={'disp': True}\n)\n\nbeta_estimates = result_poisson_reg.x\n\n         Current function value: -1790.814845\n         Iterations: 0\n         Function evaluations: 264\n         Gradient evaluations: 28\n\n\ne:\\2025 Spring - ucsd\\Sophia's Website\\quarto-env\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:733: OptimizeWarning: Desired error not necessarily achieved due to precision loss.\n  res = _minimize_bfgs(fun, x0, args, jac, callback, **options)\n\n\n\n\n\n\n\ndef hessian(func, x, epsilon=1e-5):\n    n = len(x)\n    hess = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            x1, x2, x3, x4 = x.copy(), x.copy(), x.copy(), x.copy()\n            if i == j:\n                x1[i] += epsilon\n                x2[i] -= epsilon\n                hess[i, i] = (func(x1) + func(x2) - 2 * func(x)) / (epsilon ** 2)\n            else:\n                x1[i] += epsilon; x1[j] += epsilon\n                x2[i] += epsilon; x2[j] -= epsilon\n                x3[i] -= epsilon; x3[j] += epsilon\n                x4[i] -= epsilon; x4[j] -= epsilon\n                hess[i, j] = (func(x1) + func(x4) - func(x2) - func(x3)) / (4 * epsilon ** 2)\n                hess[j, i] = hess[i, j]\n    return hess\n\nH = hessian(lambda b: poisson_regression_loglikelihood(b, y, X), beta_estimates)\ncov_matrix = np.linalg.inv(H)\nstd_errors = np.sqrt(np.diag(cov_matrix))\n\nz_values = beta_estimates / std_errors\np_values = 2 * (1 - np.abs(scipy.stats.norm.cdf(z_values)))\n\ncustom_results = pd.DataFrame({\n    'Coefficient': beta_estimates,\n    'Std. Error': std_errors,\n    'z-value': z_values,\n    'p-value': p_values\n}, index=X_data.columns)\n\ncustom_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1837\n-2.7698\n1.9944\n\n\nage\n0.1486\n0.0139\n10.6800\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.4738\n2.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7190\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n1.2562\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\n\n\n\n\n\nsm_results = pd.DataFrame({\n    'Coefficient': glm_results.params,\n    'Std. Error': glm_results.bse,\n    'z-value': glm_results.tvalues,\n    'p-value': glm_results.pvalues\n}, index=X_data.columns)\n\nsm_results.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nintercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n\n\nage\n0.1486\n0.0139\n10.7162\n0.0000\n\n\nage_squared\n-0.0030\n0.0003\n-11.5132\n0.0000\n\n\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n\n\nregion_Northeast\n0.0292\n0.0436\n0.6686\n0.5037\n\n\nregion_Northwest\n-0.0176\n0.0538\n-0.3268\n0.7438\n\n\nregion_South\n0.0566\n0.0527\n1.0740\n0.2828\n\n\nregion_Southwest\n0.0506\n0.0472\n1.0716\n0.2839\n\n\n\n\n\n\n\nThe estimates and standard errors obtained using scipy.optimize match those from statsmodels, confirming the correctness of our custom likelihood implementation.\n\n\n\nThe model successfully estimated the impact of several firm-level characteristics on patent success. we found:\n\nFirm Age: A positive and significant effect (age coefficient = 0.1486, p &lt; 0.001), with a small negative effect from age squared. This suggests diminishing returns to age: patent counts increase with age but at a decreasing rate.\nRegion: None of the region coefficients were statistically significant at the 5% level, indicating little evidence of regional differences in patent activity once other variables are controlled for.\nBlueprinty Software (iscustomer): The coefficient for iscustomer is 0.2076 with a p-value of &lt; 0.001, meaning it is both positive and highly statistically significant.\n\n\n\n\n\nTo interpret the effect of the binary variable iscustomer, we simulate two scenarios: - (X_0): all firms are non-customers (iscustomer = 0) - (X_1): all firms are customers (iscustomer = 1)\nwe compare the average predicted number of patents under each scenario:\n\nX_0 = X.copy()\nX_1 = X.copy()\niscustomer_idx = list(X_data.columns).index('iscustomer')\nX_0[:, iscustomer_idx] = 0\nX_1[:, iscustomer_idx] = 1\n\nlambda_0 = np.exp(np.dot(X_0, beta_estimates))\nlambda_1 = np.exp(np.dot(X_1, beta_estimates))\n\ndifferences = lambda_1 - lambda_0\navg_effect = np.mean(differences)\nperc_increase = 100 * avg_effect / np.mean(lambda_0)\n\nprint(\"Average predicted increase in patents:\", round(avg_effect, 3), \"per firm\")\nprint(\"Percent increase:\", round(perc_increase, 2), \"%\")\n\nAverage predicted increase in patents: 0.793 per firm\nPercent increase: 23.07 %\n\n\nThis means that, holding all else constant, firms using Blueprinty’s software are predicted to earn about 0.79 more patents over 5 years, which represents a 23% improvement compared to non-customers.\n\n\n\n\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 6))\nplt.hist(differences, bins=30, alpha=0.7)\nplt.axvline(avg_effect, color='red', linestyle='--', \n            label=f'Avg Effect: {avg_effect:.2f} patents')\nplt.xlabel('Increase in Predicted Patents')\nplt.ylabel('Number of Firms')\nplt.title('Effect of Using Blueprinty Software')\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe histogram shows that nearly all firms are expected to benefit from adopting Blueprinty’s software, reinforcing the interpretation that the product positively contributes to patenting success.\nThis result supports the company’s marketing claim, though causal inference would require further analysis beyond this observational model."
  },
  {
    "objectID": "projects/hw2/index.html#airbnb-case-study",
    "href": "projects/hw2/index.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\n\n\nData Analysis\n\n# Load the AirBnB data\nairbnb_df = pd.read_csv('airbnb.csv',index_col=0)\n\n# Display the first few rows\nairbnb_df.head()\n\n\n\n\n\n\n\n\nid\ndays\nlast_scraped\nhost_since\nroom_type\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\ninstant_bookable\n\n\n\n\n1\n2515\n3130\n4/2/2017\n9/6/2008\nPrivate room\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nf\n\n\n2\n2595\n3127\n4/2/2017\n9/9/2008\nEntire home/apt\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nf\n\n\n3\n3647\n3050\n4/2/2017\n11/25/2008\nPrivate room\n1.0\n1.0\n150\n0\nNaN\nNaN\nNaN\nf\n\n\n4\n3831\n3038\n4/2/2017\n12/7/2008\nEntire home/apt\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nf\n\n\n5\n4611\n3012\n4/2/2017\n1/2/2009\nPrivate room\nNaN\n1.0\n39\n93\n9.0\n8.0\n9.0\nt\n\n\n\n\n\n\n\n\n# Check the data structure and missing values\nairbnb_df.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nIndex: 40628 entries, 1 to 40628\nData columns (total 13 columns):\n #   Column                     Non-Null Count  Dtype  \n---  ------                     --------------  -----  \n 0   id                         40628 non-null  int64  \n 1   days                       40628 non-null  int64  \n 2   last_scraped               40628 non-null  object \n 3   host_since                 40593 non-null  object \n 4   room_type                  40628 non-null  object \n 5   bathrooms                  40468 non-null  float64\n 6   bedrooms                   40552 non-null  float64\n 7   price                      40628 non-null  int64  \n 8   number_of_reviews          40628 non-null  int64  \n 9   review_scores_cleanliness  30433 non-null  float64\n 10  review_scores_location     30374 non-null  float64\n 11  review_scores_value        30372 non-null  float64\n 12  instant_bookable           40628 non-null  object \ndtypes: float64(5), int64(4), object(4)\nmemory usage: 4.3+ MB\n\n\n\n# Summary statistics\nairbnb_df.describe()\n\n\n\n\n\n\n\n\nid\ndays\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\n\n\n\n\ncount\n4.062800e+04\n40628.000000\n40468.000000\n40552.000000\n40628.000000\n40628.000000\n30433.000000\n30374.000000\n30372.000000\n\n\nmean\n9.698889e+06\n1102.368219\n1.124592\n1.147046\n144.760732\n15.904426\n9.198370\n9.413544\n9.331522\n\n\nstd\n5.460166e+06\n1383.269358\n0.385884\n0.691746\n210.657597\n29.246009\n1.119935\n0.844949\n0.902966\n\n\nmin\n2.515000e+03\n1.000000\n0.000000\n0.000000\n10.000000\n0.000000\n2.000000\n2.000000\n2.000000\n\n\n25%\n4.889868e+06\n542.000000\n1.000000\n1.000000\n70.000000\n1.000000\n9.000000\n9.000000\n9.000000\n\n\n50%\n9.862878e+06\n996.000000\n1.000000\n1.000000\n100.000000\n4.000000\n10.000000\n10.000000\n10.000000\n\n\n75%\n1.466789e+07\n1535.000000\n1.000000\n1.000000\n170.000000\n17.000000\n10.000000\n10.000000\n10.000000\n\n\nmax\n1.800967e+07\n42828.000000\n8.000000\n10.000000\n10000.000000\n421.000000\n10.000000\n10.000000\n10.000000\n\n\n\n\n\n\n\n\n\nData Cleaning and Feature Engineering\n\n# Select relevant columns\nmodel_cols = ['room_type', 'bathrooms',\n    'bedrooms','price' ,'number_of_reviews',  \n    'review_scores_cleanliness', 'review_scores_location',\n    'review_scores_value', 'instant_bookable', \n]\n\n# Drop missing values\nairbnb_clean = airbnb_df[model_cols].dropna()\n\n# Convert categorical variables into dummy variables\nairbnb_clean = pd.get_dummies(airbnb_clean, columns=[\"room_type\", \"instant_bookable\"], drop_first=True)\n\nairbnb_clean.head()\n\n\n\n\n\n\n\n\nbathrooms\nbedrooms\nprice\nnumber_of_reviews\nreview_scores_cleanliness\nreview_scores_location\nreview_scores_value\nroom_type_Private room\nroom_type_Shared room\ninstant_bookable_t\n\n\n\n\n1\n1.0\n1.0\n59\n150\n9.0\n9.0\n9.0\nTrue\nFalse\nFalse\n\n\n2\n1.0\n0.0\n230\n20\n9.0\n10.0\n9.0\nFalse\nFalse\nFalse\n\n\n4\n1.0\n1.0\n89\n116\n9.0\n9.0\n9.0\nFalse\nFalse\nFalse\n\n\n6\n1.0\n1.0\n212\n60\n9.0\n9.0\n9.0\nFalse\nFalse\nFalse\n\n\n7\n1.0\n2.0\n250\n60\n10.0\n9.0\n10.0\nFalse\nFalse\nFalse\n\n\n\n\n\n\n\n\n\n\nFit Poisson Regression Model\n\n# Define target and features\ny = airbnb_clean[\"number_of_reviews\"]\nX = airbnb_clean.drop(columns=[\"number_of_reviews\"])\nX = sm.add_constant(X)  # Add intercept\n\nX = X.astype(float)\ny = y.astype(float)\n\n# Fit Poisson regression\nmodel = sm.GLM(y, X, family=sm.families.Poisson())\nresults = model.fit()\n\nresults.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\nnumber_of_reviews\nNo. Observations:\n30160\n\n\nModel:\nGLM\nDf Residuals:\n30150\n\n\nModel Family:\nPoisson\nDf Model:\n9\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-5.2900e+05\n\n\nDate:\nTue, 06 May 2025\nDeviance:\n9.3653e+05\n\n\nTime:\n20:53:45\nPearson chi2:\n1.41e+06\n\n\nNo. Iterations:\n6\nPseudo R-squ. (CS):\n0.5649\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nconst\n3.5725\n0.016\n223.215\n0.000\n3.541\n3.604\n\n\nbathrooms\n-0.1240\n0.004\n-33.091\n0.000\n-0.131\n-0.117\n\n\nbedrooms\n0.0749\n0.002\n37.698\n0.000\n0.071\n0.079\n\n\nprice\n-1.435e-05\n8.3e-06\n-1.729\n0.084\n-3.06e-05\n1.92e-06\n\n\nreview_scores_cleanliness\n0.1132\n0.001\n75.820\n0.000\n0.110\n0.116\n\n\nreview_scores_location\n-0.0768\n0.002\n-47.796\n0.000\n-0.080\n-0.074\n\n\nreview_scores_value\n-0.0915\n0.002\n-50.902\n0.000\n-0.095\n-0.088\n\n\nroom_type_Private room\n-0.0145\n0.003\n-5.310\n0.000\n-0.020\n-0.009\n\n\nroom_type_Shared room\n-0.2519\n0.009\n-29.229\n0.000\n-0.269\n-0.235\n\n\ninstant_bookable_t\n0.3344\n0.003\n115.748\n0.000\n0.329\n0.340\n\n\n\n\n\n\n\n\nInterpretation of Results\n\nresults_table = pd.DataFrame({\n    \"Coefficient\": results.params,\n    \"Std. Error\": results.bse,\n    \"z-value\": results.tvalues,\n    \"p-value\": results.pvalues\n})\nresults_table.round(4)\n\n\n\n\n\n\n\n\nCoefficient\nStd. Error\nz-value\np-value\n\n\n\n\nconst\n3.5725\n0.0160\n223.2145\n0.0000\n\n\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n\n\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n\n\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n\n\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n\n\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n\n\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n\n\nroom_type_Private room\n-0.0145\n0.0027\n-5.3104\n0.0000\n\n\nroom_type_Shared room\n-0.2519\n0.0086\n-29.2286\n0.0000\n\n\ninstant_bookable_t\n0.3344\n0.0029\n115.7477\n0.0000\n\n\n\n\n\n\n\n\n\nInterpretation of Poisson Regression Results\nThe number of reviews (used as a proxy for bookings) varies significantly across listing features:\n\nBedrooms (+): Listings with more bedrooms receive more reviews, likely due to larger group bookings.\nBathrooms (-): Surprisingly, more bathrooms are associated with fewer reviews, possibly due to lower turnover (longer stays).\nPrice (n.s.): The price effect is small and not statistically significant.\nReview Scores:\n\nCleanliness (+): Strong positive effect—cleaner listings attract more reviews.\nLocation & Value (-): Both are negatively associated with review counts, possibly due to selection or expectation effects.\n\nRoom Type:\n\nPrivate room (-) and Shared room (−−): Receive fewer reviews than entire homes.\n\nInstant Bookable (+): Listings that allow instant booking get significantly more reviews, likely due to ease of reservation.\n\nThese results suggest that amenities, guest experience, and booking convenience all influence booking volume on Airbnb."
  }
]